### Transformer-TabanlÄ± TÄ°D TanÄ±ma â€” 5 GÃ¼nlÃ¼k Staj Defteri

Bu rapor, `transformer-signlang` projesi kapsamÄ±nda Transformer tabanlÄ± iÅŸaret dili tanÄ±ma hattÄ±nÄ±n (MediaPipe keypoint â†’ Temporal Transformer â†’ DeÄŸerlendirme/GÃ¶rselleÅŸtirme) 5 gÃ¼nlÃ¼k staj Ã§alÄ±ÅŸmasÄ±nÄ± ayrÄ±ntÄ±lÄ± biÃ§imde anlatÄ±r. Kod referanslarÄ±, komutlar ve Ã¶rnek Ã§Ä±ktÄ± ÅŸemalarÄ± dahildir.

---

### 1) Proje Ã–zeti ve Hedefler

- **AmaÃ§**: MediaPipe Holistic ile Ã§Ä±karÄ±lan 258â€‘boyutlu anahtar nokta dizilerinden TÃ¼rk Ä°ÅŸaret Dili (TÄ°D) kelime sÄ±nÄ±flandÄ±rmasÄ± yapan bir Transformer modeli geliÅŸtirmek ve 226 kelimelik geniÅŸleme ile Ã¶lÃ§eklemek.
- **Ã‡Ä±ktÄ±lar**: EÄŸitim/deÄŸerlendirme scriptleri (`train.py`, `evaluate.py`), test videolarÄ±nda uÃ§tan uca Ã§Ä±karÄ±m (`inference_test_videos.py`), attention gÃ¶rselleÅŸtirme (`visualize_attention.py`), kayÄ±tlÄ± sonuÃ§lar (`results/`) ve checkpointâ€™ler (`checkpoints/`).

---

### 1.1) Deneme AÅŸamalarÄ± ve SÃ¼reler

- **Deneme 1 â€” 3 Kelime (POC):** `abla`, `acele`, `acikmak` sÄ±nÄ±flarÄ±yla uÃ§tan uca hattÄ±n ilk doÄŸrulamasÄ± yapÄ±ldÄ±. Veri hazÄ±rlama ve eÄŸitim hÄ±zla tamamlandÄ±; mimari doÄŸrulandÄ±.
- **Deneme 2 â€” 10 Kelime:** SÄ±nÄ±f sayÄ±sÄ± 10â€™a Ã§Ä±karÄ±larak model kapasitesi, label smoothing, dropout ve pooling stratejileri gÃ¶zlemlendi; veri boru hattÄ±nÄ±n Ã¶lÃ§eklenebilirliÄŸi test edildi.
- **Deneme 3 â€” 226 Kelime (TÃ¼m AUTSL):** TÃ¼m sÄ±nÄ±flar iÃ§in tam Ã¶lÃ§ekli deney gerÃ§ekleÅŸtirildi.
  - Keypoint extraction (MediaPipe Holistic) sÃ¼resi: **â‰ˆ 19 saat** (tÃ¼m videolarÄ±n 258â€‘boyutlu anahtar nokta Ã§Ä±karÄ±mÄ± ve kaydÄ±).
  - Model eÄŸitimi sÃ¼resi: **â‰ˆ 5 saat** (Base konfigÃ¼rasyon; optimizasyon: AdamW + cosine annealing + warmup).
  - Not: SÃ¼reler donanÄ±m (GPU/CPU), disk I/O ve eÅŸzamanlÄ± iÅŸ sayÄ±sÄ±na gÃ¶re deÄŸiÅŸebilir; raporlanan deÄŸerler bu kurulumda gÃ¶zlemlenen yaklaÅŸÄ±k sÃ¼relerdir.

### 2) GÃ¼nlÃ¼k Plan ve Ã‡Ä±ktÄ±lar

#### GÃ¼n 1 â€” LiteratÃ¼r, Mimari ve Kurulum

- Transformer yaklaÅŸÄ±mÄ±: Uzun menzilli baÄŸÄ±mlÄ±lÄ±klar, paralel eÄŸitim, attention ile yorumlanabilirlik.
- Girdi temsili: Pose(33Ã—3=99) + Face(11Ã—3=33) + Eller(2Ã—21Ã—3=126) = **258** Ã¶zellik/frame.
- Model iskeleti: InputProjection â†’ PositionalEncoding â†’ NÃ—TransformerEncoder â†’ Pooling (GAP/CLS/Last) â†’ Classifier.

Kod referansÄ± (konfigÃ¼rasyon ve hiperparametreler):
```28:58:/Users/siyaksares/Developer/GitHub/klassifier-sign-language/transformer-signlang/config.py
class TransformerConfig:
    INPUT_DIM = 258
    MAX_SEQ_LENGTH = 200
    NUM_CLASSES = 226
    D_MODEL = 256
    NHEAD = 8
    NUM_ENCODER_LAYERS = 6
    DIM_FEEDFORWARD = 1024
    DROPOUT = 0.2
    BATCH_SIZE = 16
    LEARNING_RATE = 1e-4
    LABEL_SMOOTHING = 0.15
    POOLING_TYPE = 'gap'
```

Kurulum:
```bash
cd /Users/siyaksares/Developer/GitHub/klassifier-sign-language/transformer-signlang
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt
mkdir -p data/keypoints data/processed checkpoints results logs
```

#### GÃ¼n 2 â€” Veri HazÄ±rlama: SeÃ§im, Keypoint Ã‡Ä±karÄ±mÄ±, Normalizasyon

- SeÃ§im CSVâ€™leri oluÅŸturma (train/val/test), MediaPipe ile 258â€‘boyutlu vektÃ¶r Ã§Ä±karma, scaler fit etme (yalnÄ±z train), pad/truncate ile sabit uzunluÄŸa getirme.

Ã‡erÃ§eveden 258â€‘boyut Ã§Ä±karÄ±mÄ±:
```50:96:/Users/siyaksares/Developer/GitHub/klassifier-sign-language/transformer-signlang/inference_test_videos.py
def extract_keypoints_from_frame(results):
    # Pose: 33Ã—3, Face: 11Ã—3 (seÃ§ilmiÅŸ kilit noktalar), Sol/saÄŸ el: 21Ã—3
    # BirleÅŸtir â†’ 99 + 33 + 63 + 63 = 258
    ...
```

Normalizasyon ve pad/truncate:
```147:184:/Users/siyaksares/Developer/GitHub/klassifier-sign-language/transformer-signlang/inference_test_videos.py
def normalize_sequence(sequence, scaler):
    return scaler.transform(sequence)

def pad_or_truncate_sequence(sequence, target_length):
    # Uzunsa son target_length; kÄ±saysa baÅŸa sÄ±fÄ±r pad
    ...
```

Ã‡alÄ±ÅŸtÄ±rma (Ã¶rnek):
```bash
python scripts/01_select_videos.py
python scripts/02_extract_keypoints.py
python scripts/03_normalize_data.py
```

Beklenen veriler: `data/scaler.pkl`, `data/processed/X_{train,val,test}.npy`, `y_{...}.npy`.

#### GÃ¼n 3 â€” EÄŸitim: Optimizasyon ve Takip

- Label smoothingâ€™li CE loss, AdamW + Cosine annealing (warmup), gradient clipping, early stopping.
- Cihaz seÃ§imi: CUDA > MPS > CPU; Ã¶zet ve batch istatistikleri loglanÄ±r; en iyi model `checkpoints/best_model.pth`.

EÄŸitim dÃ¶ngÃ¼sÃ¼:
```423:471:/Users/siyaksares/Developer/GitHub/klassifier-sign-language/transformer-signlang/train.py
def main():
    config = TransformerConfig()
    # Data yÃ¼kleme â†’ DataLoader
    model = TransformerSignLanguageClassifier(...)
    criterion = LabelSmoothingCrossEntropy(epsilon=config.LABEL_SMOOTHING)
    optimizer = create_optimizer(model, config)
    scheduler = create_scheduler(optimizer, config, num_training_steps)
    ...  # train_epoch / validate_epoch ve checkpoint kaydÄ±
```

Komutlar:
```bash
python train.py                     # sÄ±fÄ±rdan
python train.py --resume-from-best  # en iyi modelden devam
```

Ã–rnek epoch Ã¶zeti (konsol):
```text
ğŸ“Š Epoch 12/100 Summary:
   Train Loss: 0.8421 | Train Acc: 0.7634
   Val   Loss: 0.9153 | Val   Acc: 0.7420 | Val F1: 0.7351
   Learning Rate: 0.000073
   âœ… Best model saved! (Val Acc: 0.7420)
```

#### GÃ¼n 4 â€” DeÄŸerlendirme ve SonuÃ§larÄ±n KaydÄ±

- Test seti Ã¼zerinde accuracy, macro/micro F1, karÄ±ÅŸÄ±klÄ±k matrisi, sÄ±nÄ±f bazlÄ± metrikler, confidence daÄŸÄ±lÄ±mÄ±; gÃ¶rseller `results/` altÄ±na kaydedilir.

DeÄŸerlendirme akÄ±ÅŸÄ±:
```407:551:/Users/siyaksares/Developer/GitHub/klassifier-sign-language/transformer-signlang/evaluate.py
def main():
    config = TransformerConfig()
    X_test = np.load(...)
    model = TransformerSignLanguageClassifier(...)
    checkpoint = torch.load('checkpoints/best_model.pth')
    model.load_state_dict(checkpoint['model_state_dict'])
    all_preds, all_probs, all_targets = evaluate_model(...)
    metrics = compute_metrics(all_targets, all_preds, config.CLASS_NAMES)
    save_results(metrics, config, config.RESULTS_DIR)
    # confusion_matrix_raw/normalized.png, per_class_metrics.png, prediction_confidence.png
```

Komut:
```bash
python evaluate.py
```

Beklenen kayÄ±tlar: `evaluation_report.json`, `confusion_matrix_*.{csv,png}`, `per_class_metrics.{csv,png}`, `prediction_confidence.png`.

#### GÃ¼n 5 â€” UÃ§tan Uca Ã‡Ä±karÄ±m ve Attention GÃ¶rselleÅŸtirme

- Test videolarÄ±nda uÃ§tan uca: video â†’ MediaPipe â†’ normalizasyon â†’ pad â†’ Transformer â†’ tahmin; interaktif oynatma, sonuÃ§ CSV/JSON.
- Attention analizleri: katman/baÅŸlÄ±k bazlÄ± Ã§oklu Ä±sÄ± haritalarÄ±, ortalama attention ve rollout.

Video Ã§Ä±karÄ±mÄ±:
```457:654:/Users/siyaksares/Developer/GitHub/klassifier-sign-language/transformer-signlang/inference_test_videos.py
model, scaler, checkpoint = load_model_and_scaler(config, device)
for row in test_df.itertuples():
    result = process_and_display_video(...)
    # results/test_predictions.{csv,json}
```

Attention gÃ¶rselleÅŸtirme:
```382:522:/Users/siyaksares/Developer/GitHub/klassifier-sign-language/transformer-signlang/visualize_attention.py
extractor = AttentionExtractor(model)
attention_weights = extractor.get_attention_weights(x, mask=None)
plot_multi_head_attention(...)
plot_averaged_attention(...)
plot_attention_rollout(...)
plot_attention_statistics(...)
```

Komutlar:
```bash
python inference_test_videos.py
python visualize_attention.py --num_samples 6
```

Ã–rnek Ã¶zet istatistik (konsol):
```text
ğŸ“ˆ Genel Performans:
   - Toplam video: 150
   - DoÄŸru tahmin: 118
   - Accuracy: 78.67%
ğŸ¯ Confidence Ä°statistikleri:
   - Ortalama: 0.73
   - DoÄŸru/yanlÄ±ÅŸ ayrÄ±mÄ± belirgin, yanlÄ±ÅŸlarda ~0.48 ortalama
```

---

### 3) Teknik Notlar ve Ä°yileÅŸtirme Ã–nerileri

- Mask uyumluluÄŸu: MPS Ã¼zerinde bazÄ± mask sÄ±nÄ±rlamalarÄ±; eÄŸitimde/Ã§Ä±karÄ±mda gerekli yerlerde devre dÄ±ÅŸÄ± bÄ±rakma iÅŸ akÄ±ÅŸlarÄ± uygulanmÄ±ÅŸtÄ±r.
- Veri tarafÄ±: Scaler yalnÄ±zca train setinde fit edilmeli; pad stratejisi (pre/post) kararlÄ±lÄ±k iÃ§in sabit tutulmalÄ±.
- Hiperparametre denemeleri: `D_MODEL/NHEAD/layers`, label smoothing, dropout ve pooling tÃ¼rÃ¼; sÄ±nÄ±f dengesizliÄŸi iÃ§in aÄŸÄ±rlÄ±klÄ± loss veya focal loss alternatifleri.
- Gelecek iÅŸ: Multiâ€‘scale temporal attention, selfâ€‘supervised pretraining, ONNX/TorchScript export, gerÃ§ek zamanlÄ± pipeline optimizasyonu.

---

### 4) HÄ±zlÄ± Komut Ã–zeti

```bash
cd transformer-signlang
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# Veri hazÄ±rlama
python scripts/01_select_videos.py
python scripts/02_extract_keypoints.py
python scripts/03_normalize_data.py

# EÄŸitim
python train.py

# DeÄŸerlendirme
python evaluate.py

# UÃ§tan uca Ã§Ä±karÄ±m ve gÃ¶rselleÅŸtirme
python inference_test_videos.py
python visualize_attention.py --num_samples 6
```

---

### 5) SonuÃ§

BeÅŸ gÃ¼n sonunda, 258â€‘boyutlu MediaPipe anahtar noktalarÄ±yla beslenen Temporal Transformer modeli; eÄŸitim, deÄŸerlendirme, test videolarÄ±nda Ã§Ä±karÄ±m ve attention gÃ¶rselleÅŸtirme fonksiyonlarÄ±yla birlikte uÃ§tan uca Ã§alÄ±ÅŸÄ±r hale getirilmiÅŸtir. `results/` klasÃ¶rÃ¼ne metrikler ve gÃ¶rseller kaydedilmekte; `checkpoints/` altÄ±nda en iyi model saklanmaktadÄ±r. Ã–lÃ§eklenebilirlik ve yorumlanabilirlik hedeflerine yÃ¶nelik iyileÅŸtirme alanlarÄ± belirlenmiÅŸtir.


