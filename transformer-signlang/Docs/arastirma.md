# Ã–rnek Transformer TabanlÄ± Ä°ÅŸaret Dili TanÄ±ma Projesi Ä°ÅŸ PlanÄ±

## 1. Proje Hedefi

Bu proje, **TÃ¼rk Ä°ÅŸaret Dili (TÄ°D)** video verileri Ã¼zerinde **Transformer tabanlÄ± derin Ã¶ÄŸrenme** modeli kullanarak iÅŸaret dili tanÄ±ma sistemi geliÅŸtirmeyi amaÃ§lamaktadÄ±r.

### 1.1 Kapsam

-**Hedef Kelimeler:** Ä°lk Ã¼Ã§ kelime (abla, acele, acikmak)

-**Veri KaynaÄŸÄ±:**`TID-N/videos/` dizini altÄ±ndaki MediaPipe keypoint verileri

-**Model TÃ¼rÃ¼:** Temporal Transformer (Sequence-to-Classification)

-**Ã–zellik VektÃ¶rÃ¼:** Frame baÅŸÄ±na 258 boyutlu MediaPipe keypoint'leri

- 33 pose keypoints Ã— 3 (x, y, z) = 99 boyut
- 21 sol el keypoints Ã— 3 = 63 boyut
- 21 saÄŸ el keypoints Ã— 3 = 63 boyut
- 468 yÃ¼z keypoints (opsiyonel, DROP_FACE=True ise kullanÄ±lmaz)

### 1.2 Proje Ã‡Ä±ktÄ±larÄ±

- âœ… EÄŸitim iÃ§in hazÄ±rlanmÄ±ÅŸ veri seti (train/validation/test split)
- âœ… Transformer tabanlÄ± iÅŸaret dili tanÄ±ma modeli
- âœ… EÄŸitilmiÅŸ model checkpoint'leri
- âœ… KapsamlÄ± deÄŸerlendirme raporlarÄ± ve gÃ¶rselleÅŸtirmeler
- âœ… GerÃ§ek zamanlÄ± tahmin altyapÄ±sÄ±

---

## 2. Veri HazÄ±rlama SÃ¼reci

### 2.1 Mevcut Veri YapÄ±sÄ±

Proje dizini altÄ±nda zaten iÅŸlenmiÅŸ keypoint verileri bulunmaktadÄ±r:

```

TID-N/videos/

â”œâ”€â”€ abla/

â”‚   â”œâ”€â”€ signer0_sample1034/

â”‚   â”‚   â”œâ”€â”€ frame_0001.npy  # 258 boyutlu keypoint

â”‚   â”‚   â”œâ”€â”€ frame_0002.npy

â”‚   â”‚   â””â”€â”€ ... (deÄŸiÅŸken sayÄ±da frame)

â”‚   â”œâ”€â”€ signer0_sample1044/

â”‚   â””â”€â”€ ... (toplam ~128 Ã¶rnek)

â”œâ”€â”€ acele/

â”‚   â””â”€â”€ ... (toplam ~100 Ã¶rnek)

â””â”€â”€ acikmak/

    â””â”€â”€ ... (toplam ~107 Ã¶rnek)

```

Her `.npy` dosyasÄ± bir frame iÃ§in 258 boyutlu numpy array iÃ§erir:

-**Shape:**`(258,)` veya `(1, 258)`

-**Data Type:**`float32` veya `float64`

### 2.2 Veri YÃ¼kleme ve Organizasyon

#### 2.2.1 Keypoint DosyalarÄ±nÄ±n OkunmasÄ±

Her video Ã¶rneÄŸi iÃ§in keypoint'ler sequential olarak yÃ¼klenir:

```python

import numpy as np

import os

from glob import glob


defload_sequence_keypoints(sequence_path):

"""

    Bir video Ã¶rneÄŸine ait tÃ¼m frame keypoint'lerini yÃ¼kler


    Args:

        sequence_path: signer_sample klasÃ¶r yolu


    Returns:

        keypoints: (num_frames, 258) shape'inde numpy array

    """

# Frame dosyalarÄ±nÄ± sÄ±ralÄ± ÅŸekilde al

    frame_files =sorted(glob(os.path.join(sequence_path, "frame_*.npy")))


# Her frame'i yÃ¼kle

    frames = []

for frame_file in frame_files:

        keypoint = np.load(frame_file)

# Shape'i normalize et (258,) veya (1,258) -> (258,)

if keypoint.ndim >1:

            keypoint = keypoint.flatten()

        frames.append(keypoint)


# (num_frames, 258) shape'inde array oluÅŸtur

    keypoints = np.array(frames, dtype=np.float32)

return keypoints

```

#### 2.2.2 TÃ¼m Veri Setinin OluÅŸturulmasÄ±

```python

defbuild_dataset(video_root, class_names):

"""

    TÃ¼m video Ã¶rneklerini yÃ¼kleyip etiketleriyle eÅŸleÅŸtirir


    Args:

        video_root: TID-N/videos/ dizini

        class_names: ['abla', 'acele', 'acikmak']


    Returns:

        sequences: List of (num_frames, 258) arrays

        labels: List of integer class labels

        metadata: List of dicts with signer/sample info

    """

    sequences = []

    labels = []

    metadata = []


for class_id, class_name inenumerate(class_names):

        class_path = os.path.join(video_root, class_name)


# TÃ¼m signer_sample klasÃ¶rlerini bul

        sample_dirs =sorted(glob(os.path.join(class_path, "signer*_sample*")))


print(f"[{class_name}] {len(sample_dirs)} Ã¶rnek bulundu")


for sample_dir in sample_dirs:

try:

# Keypoint'leri yÃ¼kle

                keypoints = load_sequence_keypoints(sample_dir)


# Ã‡ok kÄ±sa veya Ã§ok uzun sekanslarÄ± filtrele

if keypoints.shape[0] <10or keypoints.shape[0] >200:

print(f"âš ï¸ Filtrelendi: {sample_dir} (frame count: {keypoints.shape[0]})")

continue


                sequences.append(keypoints)

                labels.append(class_id)


# Metadata ekle (debugging iÃ§in)

                sample_name = os.path.basename(sample_dir)

                metadata.append({

'class_name': class_name,

'class_id': class_id,

'sample_name': sample_name,

'num_frames': keypoints.shape[0]

                })


exceptExceptionas e:

print(f"âŒ Hata: {sample_dir} - {str(e)}")

continue


return sequences, labels, metadata

```

### 2.3 Sekans UzunluÄŸu Normalizasyonu

Transformer modelleri sabit uzunlukta giriÅŸ bekler. FarklÄ± uzunluklardaki videolarÄ± normalize etmek iÃ§in iki yÃ¶ntem:

#### 2.3.1 YÃ¶ntem 1: Padding/Truncation (Ã–nerilen)

```python

defnormalize_sequence_length(sequences, target_length=60, mode='pad'):

"""

    SekanslarÄ± hedef uzunluÄŸa normalize eder


    Args:

        sequences: List of (num_frames, 258) arrays

        target_length: Hedef frame sayÄ±sÄ±

        mode: 'pad' (padding) veya 'interpolate' (yeniden Ã¶rnekleme)


    Returns:

        normalized: (num_samples, target_length, 258) array

        masks: (num_samples, target_length) binary mask (padding tespiti iÃ§in)

    """

    normalized = []

    masks = []


for seq in sequences:

        num_frames = seq.shape[0]


if mode =='pad':

if num_frames >= target_length:

# Truncate: Ä°lk target_length frame'i al

                new_seq = seq[:target_length]

                mask = np.ones(target_length, dtype=np.float32)

else:

# Pad: SÄ±fÄ±rlarla doldur

                pad_length = target_length - num_frames

                new_seq = np.vstack([seq, np.zeros((pad_length, 258), dtype=np.float32)])

                mask = np.concatenate([np.ones(num_frames), np.zeros(pad_length)], dtype=np.float32)


elif mode =='interpolate':

# Temporal interpolation (her frame'i yeniden Ã¶rnekle)

from scipy.interpolate import interp1d

            old_indices = np.linspace(0, num_frames -1, num_frames)

            new_indices = np.linspace(0, num_frames -1, target_length)


            interpolator = interp1d(old_indices, seq, axis=0, kind='linear')

            new_seq = interpolator(new_indices)

            mask = np.ones(target_length, dtype=np.float32)


        normalized.append(new_seq)

        masks.append(mask)


return np.array(normalized, dtype=np.float32), np.array(masks, dtype=np.float32)

```

**Ã–nerilen Parametre:**

-`target_length = 60` (config.py'deki SEQ_LEN ile uyumlu)

-`mode = 'pad'` (Transformer'lar masking ile padding'i doÄŸal olarak destekler)

#### 2.3.2 YÃ¶ntem 2: Temporal Interpolation

Video'nun hÄ±zÄ±na gÃ¶re yeniden Ã¶rnekleme yapar. Daha smooth ama hesaplama maliyeti yÃ¼ksek.

### 2.4 Veri Normalizasyonu

Keypoint koordinatlarÄ±nÄ± normalize etmek model stabilitesi iÃ§in kritik:

```python

defnormalize_keypoints(sequences, method='z-score'):

"""

    Keypoint deÄŸerlerini normalize eder


    Args:

        sequences: (num_samples, target_length, 258) array

        method: 'z-score' veya 'min-max'


    Returns:

        normalized_sequences: Normalize edilmiÅŸ array

        stats: {mean, std} veya {min, max} (inference iÃ§in gerekli)

    """

if method =='z-score':

# TÃ¼m veri seti Ã¼zerinden mean ve std hesapla

        mean = sequences.mean(axis=(0, 1), keepdims=True)  # (1, 1, 258)

        std = sequences.std(axis=(0, 1), keepdims=True) +1e-8


        normalized = (sequences - mean) / std

        stats = {'mean': mean, 'std': std}


elif method =='min-max':

        min_val = sequences.min(axis=(0, 1), keepdims=True)

        max_val = sequences.max(axis=(0, 1), keepdims=True)


        normalized = (sequences - min_val) / (max_val - min_val +1e-8)

        stats = {'min': min_val, 'max': max_val}


return normalized, stats

```

**âš ï¸ Ã–nemli:** Normalizasyon istatistikleri (`stats`) mutlaka kaydedilmelidir! Inference sÄ±rasÄ±nda yeni videolar aynÄ± istatistiklerle normalize edilecek.

### 2.5 Veri Setinin BÃ¶lÃ¼mlenmesi (Train/Val/Test Split)

```python

from sklearn.model_selection import train_test_split


defsplit_dataset(sequences, labels, metadata, 

                  train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,

                  stratify=True, random_seed=42):

"""

    Veri setini train/val/test olarak bÃ¶ler


    Args:

        sequences: (num_samples, seq_len, 258) array

        labels: (num_samples,) array

        metadata: List of dicts

        train_ratio: EÄŸitim seti oranÄ± (0.8 = %80)

        val_ratio: DoÄŸrulama seti oranÄ± (0.1 = %10)

        test_ratio: Test seti oranÄ± (0.1 = %10)

        stratify: Her sÄ±nÄ±ftan eÅŸit oranda Ã¶rnek al

        random_seed: Reproducibility iÃ§in seed


    Returns:

        train_data: (X_train, y_train, meta_train)

        val_data: (X_val, y_val, meta_val)

        test_data: (X_test, y_test, meta_test)

    """

assertabs(train_ratio + val_ratio + test_ratio -1.0) <1e-6, "Oranlar toplamÄ± 1 olmalÄ±"


# Ä°lk split: train vs (val+test)

    X_train, X_temp, y_train, y_temp, meta_train, meta_temp = train_test_split(

        sequences, labels, metadata,

test_size=(1- train_ratio),

stratify=labels if stratify elseNone,

random_state=random_seed

    )


# Ä°kinci split: val vs test

    val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)

    X_val, X_test, y_val, y_test, meta_val, meta_test = train_test_split(

        X_temp, y_temp, meta_temp,

test_size=(1- val_ratio_adjusted),

stratify=y_temp if stratify elseNone,

random_state=random_seed

    )


# Split istatistiklerini yazdÄ±r

print("\nğŸ“Š Veri Seti BÃ¶lÃ¼nme Ä°statistikleri:")

print(f"  Train: {len(X_train)} Ã¶rnek ({train_ratio*100:.0f}%)")

print(f"  Val:   {len(X_val)} Ã¶rnek ({val_ratio*100:.0f}%)")

print(f"  Test:  {len(X_test)} Ã¶rnek ({test_ratio*100:.0f}%)")


# SÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ±nÄ± kontrol et

print("\nğŸ“ˆ SÄ±nÄ±f DaÄŸÄ±lÄ±mlarÄ±:")

for split_name, split_labels in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:

        unique, counts = np.unique(split_labels, return_counts=True)

print(f"  {split_name}:")

forcls, cnt inzip(unique, counts):

print(f"    Class {cls}: {cnt} Ã¶rnek")


return (X_train, y_train, meta_train), (X_val, y_val, meta_val), (X_test, y_test, meta_test)

```

**Ã–nerilen Split OranlarÄ±:**

-**Train:** 80% (~268 Ã¶rnek)

-**Validation:** 10% (~34 Ã¶rnek)

-**Test:** 10% (~34 Ã¶rnek)

### 2.6 Veri Kaydetme ve YÃ¼kleme

HazÄ±rlanan veriyi disk'e kaydetmek eÄŸitim sÃ¼recini hÄ±zlandÄ±rÄ±r:

```python

import pickle


defsave_processed_dataset(output_dir, train_data, val_data, test_data, 

                           norm_stats, class_names):

"""

    Ä°ÅŸlenmiÅŸ veri setini disk'e kaydeder


    Args:

        output_dir: Kaydedilecek dizin (Ã¶rn: TID-N/processed_data/)

        train_data, val_data, test_data: (X, y, meta) tuples

        norm_stats: Normalizasyon istatistikleri

        class_names: ['abla', 'acele', 'acikmak']

    """

    os.makedirs(output_dir, exist_ok=True)


# Veri setlerini kaydet

    np.save(os.path.join(output_dir, 'X_train.npy'), train_data[0])

    np.save(os.path.join(output_dir, 'y_train.npy'), train_data[1])


    np.save(os.path.join(output_dir, 'X_val.npy'), val_data[0])

    np.save(os.path.join(output_dir, 'y_val.npy'), val_data[1])


    np.save(os.path.join(output_dir, 'X_test.npy'), test_data[0])

    np.save(os.path.join(output_dir, 'y_test.npy'), test_data[1])


# Metadata ve config kaydet

withopen(os.path.join(output_dir, 'metadata.pkl'), 'wb') as f:

        pickle.dump({

'train_meta': train_data[2],

'val_meta': val_data[2],

'test_meta': test_data[2],

'norm_stats': norm_stats,

'class_names': class_names

        }, f)


print(f"âœ… Veri seti baÅŸarÄ±yla kaydedildi: {output_dir}")


defload_processed_dataset(data_dir):

"""

    KaydedilmiÅŸ veri setini yÃ¼kler


    Returns:

        train_data, val_data, test_data, metadata_dict

    """

    X_train = np.load(os.path.join(data_dir, 'X_train.npy'))

    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))


    X_val = np.load(os.path.join(data_dir, 'X_val.npy'))

    y_val = np.load(os.path.join(data_dir, 'y_val.npy'))


    X_test = np.load(os.path.join(data_dir, 'X_test.npy'))

    y_test = np.load(os.path.join(data_dir, 'y_test.npy'))


withopen(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:

        metadata = pickle.load(f)


return (X_train, y_train), (X_val, y_val), (X_test, y_test), metadata

```

### 2.7 Veri HazÄ±rlama Pipeline'Ä± - Ana Script

TÃ¼m adÄ±mlarÄ± bir araya getiren `prepare_data.py` scripti:

```python

# prepare_data.py

import os

import numpy as np

from config importVIDEOS_ROOT, CLASS_NAMES, SEQ_LEN


defmain():

print("ğŸš€ Transformer Veri HazÄ±rlama Pipeline BaÅŸladÄ±")

print("="*60)


# 1. Ham keypoint'leri yÃ¼kle

print("\n[1/6] Keypoint'ler yÃ¼kleniyor...")

    sequences, labels, metadata = build_dataset(VIDEOS_ROOT, CLASS_NAMES)

print(f"âœ… Toplam {len(sequences)} Ã¶rnek yÃ¼klendi")


# 2. Sekans uzunluklarÄ±nÄ± normalize et

print(f"\n[2/6] Sekanslar {SEQ_LEN} frame'e normalize ediliyor...")

    sequences_norm, masks = normalize_sequence_length(sequences, target_length=SEQ_LEN)

print(f"âœ… Shape: {sequences_norm.shape}, Masks: {masks.shape}")


# 3. Keypoint normalizasyonu

print("\n[3/6] Keypoint normalizasyonu yapÄ±lÄ±yor...")

    sequences_norm, norm_stats = normalize_keypoints(sequences_norm, method='z-score')

print(f"âœ… Mean: {norm_stats['mean'].shape}, Std: {norm_stats['std'].shape}")


# 4. Train/Val/Test split

print("\n[4/6] Veri seti bÃ¶lÃ¼nÃ¼yor...")

    train_data, val_data, test_data = split_dataset(

        sequences_norm, np.array(labels), metadata,

train_ratio=0.8, val_ratio=0.1, test_ratio=0.1

    )


# 5. Disk'e kaydet

print("\n[5/6] Veri seti disk'e kaydediliyor...")

    output_dir = os.path.join(os.path.dirname(__file__), 'processed_data')

    save_processed_dataset(output_dir, train_data, val_data, test_data, 

                          norm_stats, CLASS_NAMES)


# 6. Ã–zet istatistikler

print("\n[6/6] Veri hazÄ±rlama tamamlandÄ±!")

print("="*60)

print(f"ğŸ“ Kaydedilen dizin: {output_dir}")

print(f"ğŸ“Š Train: {train_data[0].shape}")

print(f"ğŸ“Š Val:   {val_data[0].shape}")

print(f"ğŸ“Š Test:  {test_data[0].shape}")


if__name__=="__main__":

    main()

```

**KullanÄ±m:**

```bash

cdTID-N

pythonprepare_data.py

```

**Ã‡Ä±ktÄ± Dosya YapÄ±sÄ±:**

```

TID-N/processed_data/

â”œâ”€â”€ X_train.npy          # (268, 60, 258)

â”œâ”€â”€ y_train.npy          # (268,)

â”œâ”€â”€ X_val.npy            # (34, 60, 258)

â”œâ”€â”€ y_val.npy            # (34,)

â”œâ”€â”€ X_test.npy           # (34, 60, 258)

â”œâ”€â”€ y_test.npy           # (34,)

â””â”€â”€ metadata.pkl         # normalization stats + class names

```

---

## 3. Model Mimarisi

### 3.1 Transformer Mimarisi Genel BakÄ±ÅŸ

Transformer modeli, video sekanslarÄ±ndaki temporal (zamansal) baÄŸÄ±mlÄ±lÄ±klarÄ± yakalamak iÃ§in **Multi-Head Self-Attention** mekanizmasÄ±nÄ± kullanÄ±r.

```

Input Keypoints (60, 258)

        â†“

[Input Projection] â†’ (60, d_model)

        â†“

[Positional Encoding] â†’ (60, d_model)

        â†“

[Transformer Encoder Block 1]

        â†“

[Transformer Encoder Block 2]

        â†“

[Transformer Encoder Block N]

        â†“

[Global Average Pooling] â†’ (d_model,)

        â†“

[Classification Head] â†’ (3,)

        â†“

Output: Softmax probabilities

```

### 3.2 Model BileÅŸenleri (PyTorch)

#### 3.2.1 Input Projection Layer

Keypoint vektÃ¶rlerini Transformer'Ä±n iÃ§ boyutuna (`d_model`) project eder:

```python

import torch

import torch.nn as nn


classInputProjection(nn.Module):

def__init__(self, input_dim=258, d_model=256, dropout=0.1):

"""

        Args:

            input_dim: Keypoint boyutu (258)

            d_model: Transformer hidden dim (256, 512, etc.)

            dropout: Regularization

        """

super().__init__()

self.projection = nn.Linear(input_dim, d_model)

self.dropout = nn.Dropout(dropout)

self.norm = nn.LayerNorm(d_model)


defforward(self, x):

# x: (batch, seq_len, 258)

        x =self.projection(x)  # (batch, seq_len, d_model)

        x =self.norm(x)

        x =self.dropout(x)

return x

```

#### 3.2.2 Positional Encoding

Transformer'lar sequential bilgiyi doÄŸal olarak yakalayamaz, bu nedenle pozisyon bilgisi eklenir:

```python

classPositionalEncoding(nn.Module):

def__init__(self, d_model=256, max_len=100, dropout=0.1):

"""

        Sinusoidal Positional Encoding


        PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))

        PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

        """

super().__init__()

self.dropout = nn.Dropout(dropout)


# Pozisyonel encoding tablosunu oluÅŸtur

        pe = torch.zeros(max_len, d_model)

        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        div_term = torch.exp(torch.arange(0, d_model, 2).float() *

                            (-np.log(10000.0) / d_model))


        pe[:, 0::2] = torch.sin(position * div_term)

        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)


# Buffer olarak kaydet (trainable deÄŸil)

self.register_buffer('pe', pe)


defforward(self, x):

# x: (batch, seq_len, d_model)

        x = x +self.pe[:, :x.size(1), :]

returnself.dropout(x)

```

**Alternatif:** Learnable positional embeddings (parametre sayÄ±sÄ±nÄ± artÄ±rÄ±r):

```python

self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model))

x = x +self.pos_embedding[:, :x.size(1), :]

```

#### 3.2.3 Transformer Encoder Block

Her Transformer block iÃ§erir:

1.**Multi-Head Self-Attention**

2.**Feed-Forward Network**

3.**Residual Connections + Layer Normalization**

```python

classTransformerEncoderBlock(nn.Module):

def__init__(self, d_model=256, num_heads=8, d_ff=1024, dropout=0.1):

"""

        Args:

            d_model: Model dimensiyonu

            num_heads: Attention head sayÄ±sÄ± (d_model % num_heads == 0 olmalÄ±)

            d_ff: Feed-forward hidden dim (genelde 4 * d_model)

            dropout: Dropout oranÄ±

        """

super().__init__()


# Multi-Head Self-Attention

self.self_attn = nn.MultiheadAttention(

embed_dim=d_model,

num_heads=num_heads,

dropout=dropout,

batch_first=True# (batch, seq, feature) formatÄ± iÃ§in

        )


# Feed-Forward Network

self.ffn = nn.Sequential(

            nn.Linear(d_model, d_ff),

            nn.GELU(),  # GELU aktivasyonu (ReLU'dan daha smooth)

            nn.Dropout(dropout),

            nn.Linear(d_ff, d_model)

        )


# Layer Normalization (Pre-LN yapÄ±sÄ± daha stabil)

self.norm1 = nn.LayerNorm(d_model)

self.norm2 = nn.LayerNorm(d_model)


self.dropout1 = nn.Dropout(dropout)

self.dropout2 = nn.Dropout(dropout)


defforward(self, x, mask=None):

"""

        Args:

            x: (batch, seq_len, d_model)

            mask: (batch, seq_len) - True/1 for valid tokens, False/0 for padding


        Returns:

            x: (batch, seq_len, d_model)

        """

# Self-Attention bloÄŸu (Pre-LN)

        residual = x

        x =self.norm1(x)


# Attention mask'i PyTorch formatÄ±na Ã§evir (opsiyonel)

        attn_mask =None

if mask isnotNone:

# mask: (batch, seq_len) -> attn_mask: (batch, seq_len)

# False/0 olan yerlere -inf atanÄ±r (attention'da kullanÄ±lmasÄ±n)

            attn_mask =~mask.bool()  # Invert: padding=True, valid=False


        x_attn, _ =self.self_attn(x, x, x, key_padding_mask=attn_mask)

        x = residual +self.dropout1(x_attn)


# Feed-Forward bloÄŸu (Pre-LN)

        residual = x

        x =self.norm2(x)

        x_ffn =self.ffn(x)

        x = residual +self.dropout2(x_ffn)


return x

```

#### 3.2.4 Classification Head

Transformer encoder Ã§Ä±ktÄ±sÄ±nÄ± sÄ±nÄ±f olasÄ±lÄ±klarÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r:

```python

classClassificationHead(nn.Module):

def__init__(self, d_model=256, num_classes=3, dropout=0.5, pooling='mean'):

"""

        Args:

            d_model: Transformer Ã§Ä±ktÄ± boyutu

            num_classes: SÄ±nÄ±f sayÄ±sÄ± (3: abla, acele, acikmak)

            dropout: Regularization

            pooling: 'mean', 'max', 'cls' (CLS token kullanÄ±mÄ±)

        """

super().__init__()

self.pooling = pooling


self.classifier = nn.Sequential(

            nn.LayerNorm(d_model),

            nn.Dropout(dropout),

            nn.Linear(d_model, d_model //2),

            nn.GELU(),

            nn.Dropout(dropout),

            nn.Linear(d_model //2, num_classes)

        )


defforward(self, x, mask=None):

"""

        Args:

            x: (batch, seq_len, d_model)

            mask: (batch, seq_len) - padding mask


        Returns:

            logits: (batch, num_classes)

        """

# Temporal pooling

ifself.pooling =='mean':

# Masking-aware average pooling

if mask isnotNone:

                mask_expanded = mask.unsqueeze(-1)  # (batch, seq_len, 1)

                x_sum = (x * mask_expanded).sum(dim=1)  # (batch, d_model)

                x_mean = x_sum / mask_expanded.sum(dim=1).clamp(min=1)  # Avoid div by zero

else:

                x_mean = x.mean(dim=1)

            x = x_mean


elifself.pooling =='max':

            x, _ = x.max(dim=1)


elifself.pooling =='cls':

# Ä°lk token'Ä± CLS token olarak kullan

            x = x[:, 0, :]


# Classification

        logits =self.classifier(x)  # (batch, num_classes)

return logits

```

#### 3.2.5 Tam Transformer Model

TÃ¼m bileÅŸenleri bir araya getiren ana model:

```python

classSignLanguageTransformer(nn.Module):

def__init__(self, 

                 input_dim=258,

                 d_model=256,

                 num_heads=8,

                 num_layers=6,

                 d_ff=1024,

                 num_classes=3,

                 max_seq_len=100,

                 dropout=0.1,

                 pooling='mean'):

"""

        Transformer tabanlÄ± iÅŸaret dili tanÄ±ma modeli


        Args:

            input_dim: Keypoint boyutu (258)

            d_model: Transformer hidden dim

            num_heads: Attention head sayÄ±sÄ±

            num_layers: Encoder block sayÄ±sÄ±

            d_ff: Feed-forward hidden dim

            num_classes: SÄ±nÄ±f sayÄ±sÄ±

            max_seq_len: Maksimum sekans uzunluÄŸu

            dropout: Dropout oranÄ±

            pooling: Temporal pooling stratejisi

        """

super().__init__()


# Input projection

self.input_proj = InputProjection(input_dim, d_model, dropout)


# Positional encoding

self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)


# Transformer encoder blocks

self.encoder_layers = nn.ModuleList([

            TransformerEncoderBlock(d_model, num_heads, d_ff, dropout)

for _ inrange(num_layers)

        ])


# Classification head

self.classifier = ClassificationHead(d_model, num_classes, dropout, pooling)


defforward(self, x, mask=None):

"""

        Args:

            x: (batch, seq_len, input_dim=258)

            mask: (batch, seq_len) - binary mask (1=valid, 0=padding)


        Returns:

            logits: (batch, num_classes)

        """

# Input projection: (batch, seq_len, 258) -> (batch, seq_len, d_model)

        x =self.input_proj(x)


# Positional encoding

        x =self.pos_encoding(x)


# Transformer encoder layers

for encoder inself.encoder_layers:

            x = encoder(x, mask)


# Classification

        logits =self.classifier(x, mask)


return logits


defget_attention_weights(self, x, mask=None):

"""

        Attention haritalarÄ±nÄ± gÃ¶rselleÅŸtirme iÃ§in Ã§Ä±kart

        """

# Bu fonksiyon analysis iÃ§in kullanÄ±lÄ±r (opsiyonel)

pass

```

### 3.3 Model KonfigÃ¼rasyonlarÄ±

FarklÄ± model boyutlarÄ± iÃ§in Ã¶nerilen hiperparametreler:

#### 3.3.1 Small Model (HÄ±zlÄ± deney iÃ§in)

```python

model = SignLanguageTransformer(

input_dim=258,

d_model=128,

num_heads=4,

num_layers=3,

d_ff=512,

num_classes=3,

dropout=0.1

)

# Parametre sayÄ±sÄ±: ~500K

```

#### 3.3.2 Base Model (Ã–nerilen)

```python

model = SignLanguageTransformer(

input_dim=258,

d_model=256,

num_heads=8,

num_layers=6,

d_ff=1024,

num_classes=3,

dropout=0.15

)

# Parametre sayÄ±sÄ±: ~5M

```

#### 3.3.3 Large Model (Daha fazla veri iÃ§in)

```python

model = SignLanguageTransformer(

input_dim=258,

d_model=512,

num_heads=8,

num_layers=8,

d_ff=2048,

num_classes=3,

dropout=0.2

)

# Parametre sayÄ±sÄ±: ~25M

```

### 3.4 TensorFlow/Keras Alternatifi

PyTorch yerine TensorFlow tercih ederseniz:

```python

import tensorflow as tf

from tensorflow import keras


defcreate_transformer_model(seq_len=60, input_dim=258, d_model=256, 

                             num_heads=8, num_layers=6, num_classes=3):

"""

    Keras ile Transformer model

    """

# Input

    inputs = keras.Input(shape=(seq_len, input_dim))

    mask_input = keras.Input(shape=(seq_len,), dtype='bool')


# Input projection

    x = keras.layers.Dense(d_model)(inputs)

    x = keras.layers.LayerNormalization()(x)


# Positional encoding (learnable)

    pos_embedding = keras.layers.Embedding(seq_len, d_model)(

        tf.range(seq_len)

    )

    x = x + pos_embedding


# Transformer blocks

for _ inrange(num_layers):

# Multi-head attention

        attn_output = keras.layers.MultiHeadAttention(

num_heads=num_heads, key_dim=d_model // num_heads

        )(x, x, attention_mask=mask_input)

        x = keras.layers.Add()([x, attn_output])

        x = keras.layers.LayerNormalization()(x)


# Feed-forward

        ffn = keras.Sequential([

            keras.layers.Dense(d_model *4, activation='gelu'),

            keras.layers.Dense(d_model)

        ])

        ffn_output = ffn(x)

        x = keras.layers.Add()([x, ffn_output])

        x = keras.layers.LayerNormalization()(x)


# Global pooling

    x = keras.layers.GlobalAveragePooling1D()(x)


# Classification head

    x = keras.layers.Dropout(0.5)(x)

    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)


    model = keras.Model(inputs=[inputs, mask_input], outputs=outputs)

return model

```

---

## 4. EÄŸitim Stratejisi

### 4.1 Loss Fonksiyonu

#### 4.1.1 Cross-Entropy Loss (Standart)

Ã‡ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma iÃ§in standart loss:

```python

criterion = nn.CrossEntropyLoss()

```

**Matematiksel FormÃ¼l:**

```

L = -Î£ y_true_i * log(y_pred_i)

```

#### 4.1.2 Label Smoothing

Overconfidence'Ä± Ã¶nlemek iÃ§in yumuÅŸatÄ±lmiÅŸ etiketler:

```python

criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

```

Etiketler: `[1, 0, 0]` â†’ `[0.93, 0.033, 0.033]`

#### 4.1.3 Focal Loss (Class Imbalance iÃ§in)

Dengesiz sÄ±nÄ±flar varsa (abla sÄ±nÄ±fÄ± daha fazla):

```python

classFocalLoss(nn.Module):

def__init__(self, alpha=None, gamma=2.0):

"""

        Args:

            alpha: Class weights [w0, w1, w2]

            gamma: Focusing parameter (yÃ¼ksek = zor Ã¶rneklere focus)

        """

super().__init__()

self.alpha = alpha

self.gamma = gamma


defforward(self, inputs, targets):

        ce_loss = F.cross_entropy(inputs, targets, reduction='none')

        pt = torch.exp(-ce_loss)

        focal_loss = ((1- pt) **self.gamma) * ce_loss


ifself.alpha isnotNone:

            alpha_t =self.alpha[targets]

            focal_loss = alpha_t * focal_loss


return focal_loss.mean()


# KullanÄ±m

criterion = FocalLoss(alpha=torch.tensor([1.5, 1.0, 1.0]), gamma=2.0)

```

### 4.2 Optimizer

#### 4.2.1 AdamW (Ã–nerilen)

Weight decay ile dÃ¼zenlenmiÅŸ Adam:

```python

from torch.optim import AdamW


optimizer = AdamW(

    model.parameters(),

lr=1e-4,              # Learning rate

betas=(0.9, 0.999),   # Momentum parametreleri

weight_decay=0.01,    # L2 regularization

eps=1e-8

)

```

**Learning Rate Ã–nerileri:**

- Small model: `1e-3`
- Base model: `5e-4` veya `1e-4`
- Large model: `1e-4` veya `5e-5`

#### 4.2.2 Adam (Alternatif)

```python

optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

```

### 4.3 Learning Rate Scheduler

EÄŸitim ilerledikÃ§e learning rate'i azaltmak:

#### 4.3.1 ReduceLROnPlateau (Adaptif)

```python

from torch.optim.lr_scheduler import ReduceLROnPlateau


scheduler = ReduceLROnPlateau(

    optimizer,

mode='min',         # 'min' for loss, 'max' for accuracy

factor=0.5,         # LR'yi yarÄ±ya indir

patience=10,        # 10 epoch'ta iyileÅŸme yoksa

verbose=True,

min_lr=1e-7

)


# Her epoch sonunda:

scheduler.step(val_loss)

```

#### 4.3.2 Cosine Annealing (DÃ¶ngÃ¼sel)

```python

from torch.optim.lr_scheduler import CosineAnnealingLR


scheduler = CosineAnnealingLR(

    optimizer,

T_max=50,          # 50 epoch'luk dÃ¶ngÃ¼

eta_min=1e-6# Minimum LR

)


# Her epoch sonunda:

scheduler.step()

```

#### 4.3.3 Warmup + Cosine Decay (En Ä°yi)

```python

defget_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):

"""

    Ä°lk birkaÃ§ epoch warmup, sonra cosine decay

    """

deflr_lambda(current_step):

if current_step < num_warmup_steps:

returnfloat(current_step) /float(max(1, num_warmup_steps))

        progress =float(current_step - num_warmup_steps) /float(max(1, num_training_steps - num_warmup_steps))

returnmax(0.0, 0.5* (1.0+ math.cos(math.pi * progress)))


return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


# KullanÄ±m

num_epochs =100

steps_per_epoch =len(train_loader)

scheduler = get_cosine_schedule_with_warmup(

    optimizer,

num_warmup_steps=5* steps_per_epoch,  # 5 epoch warmup

num_training_steps=num_epochs * steps_per_epoch

)


# Her batch sonunda:

scheduler.step()

```

### 4.4 Data Augmentation (Opsiyonel)

Keypoint verileri iÃ§in augmentation teknikleri:

```python

classKeypointAugmentation:

"""

    Temporal ve spatial augmentation

    """

def__init__(self, 

                 temporal_jitter=0.1,

                 spatial_noise=0.02,

                 rotation_angle=5,

                 scale_range=(0.9, 1.1)):

self.temporal_jitter = temporal_jitter

self.spatial_noise = spatial_noise

self.rotation_angle = rotation_angle

self.scale_range = scale_range


def__call__(self, keypoints):

"""

        Args:

            keypoints: (seq_len, 258)

        Returns:

            augmented: (seq_len, 258)

        """

        keypoints = keypoints.copy()


# 1. Temporal jittering (frame'leri hafif kaydÄ±r)

if np.random.rand() <0.5:

            shift =int(len(keypoints) *self.temporal_jitter * np.random.randn())

            keypoints = np.roll(keypoints, shift, axis=0)


# 2. Gaussian noise (spatial)

if np.random.rand() <0.5:

            noise = np.random.normal(0, self.spatial_noise, keypoints.shape)

            keypoints = keypoints + noise


# 3. Scaling

if np.random.rand() <0.5:

            scale = np.random.uniform(*self.scale_range)

            keypoints = keypoints * scale


# 4. Horizontal flip (x koordinatlarÄ±nÄ± tersine Ã§evir)

if np.random.rand() <0.3:

# Her 3. boyut x koordinatÄ± (0, 3, 6, ...)

            keypoints[:, 0::3] =1.0- keypoints[:, 0::3]

# Sol ve saÄŸ eli deÄŸiÅŸtir (99:162 ile 162:225)

            left_hand = keypoints[:, 99:162].copy()

            right_hand = keypoints[:, 162:225].copy()

            keypoints[:, 99:162] = right_hand

            keypoints[:, 162:225] = left_hand


return keypoints

```

**âš ï¸ Dikkat:** Augmentation'Ä± sadece training sÄ±rasÄ±nda uygulayÄ±n, validation/test'te deÄŸil.

### 4.5 Training Loop

Ana eÄŸitim dÃ¶ngÃ¼sÃ¼:

```python

import torch

from torch.utils.data import DataLoader, TensorDataset

from tqdm import tqdm


deftrain_one_epoch(model, train_loader, criterion, optimizer, device, scheduler=None):

"""

    Bir epoch eÄŸitim


    Returns:

        avg_loss, avg_acc

    """

    model.train()

    total_loss =0

    total_correct =0

    total_samples =0


    pbar = tqdm(train_loader, desc='Training')

for batch_idx, (data, targets, masks) inenumerate(pbar):

        data = data.to(device)       # (batch, 60, 258)

        targets = targets.to(device)  # (batch,)

        masks = masks.to(device)      # (batch, 60)


# Forward pass

        optimizer.zero_grad()

        outputs = model(data, masks)  # (batch, num_classes)

        loss = criterion(outputs, targets)


# Backward pass

        loss.backward()


# Gradient clipping (stability iÃ§in)

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)


        optimizer.step()


if scheduler isnotNone:

            scheduler.step()  # Batch-level scheduler iÃ§in


# Metrics

        _, predicted = outputs.max(1)

        total_loss += loss.item() * data.size(0)

        total_correct += predicted.eq(targets).sum().item()

        total_samples += data.size(0)


# Progress bar gÃ¼ncelle

        pbar.set_postfix({

'loss': f'{loss.item():.4f}',

'acc': f'{100. * total_correct / total_samples:.2f}%'

        })


    avg_loss = total_loss / total_samples

    avg_acc =100. * total_correct / total_samples


return avg_loss, avg_acc


defvalidate(model, val_loader, criterion, device):

"""

    Validation


    Returns:

        avg_loss, avg_acc

    """

    model.eval()

    total_loss =0

    total_correct =0

    total_samples =0


with torch.no_grad():

for data, targets, masks in tqdm(val_loader, desc='Validation'):

            data = data.to(device)

            targets = targets.to(device)

            masks = masks.to(device)


            outputs = model(data, masks)

            loss = criterion(outputs, targets)


            _, predicted = outputs.max(1)

            total_loss += loss.item() * data.size(0)

            total_correct += predicted.eq(targets).sum().item()

            total_samples += data.size(0)


    avg_loss = total_loss / total_samples

    avg_acc =100. * total_correct / total_samples


return avg_loss, avg_acc

```

### 4.6 Full Training Script

```python

# train.py

import torch

from torch.utils.data import DataLoader, TensorDataset


defmain():

# KonfigÃ¼rasyon

DEVICE= torch.device('cuda'if torch.cuda.is_available() else'cpu')

BATCH_SIZE=32

NUM_EPOCHS=100

LEARNING_RATE=1e-4


print(f"ğŸš€ Training baÅŸlÄ±yor - Device: {DEVICE}")

print("="*60)


# 1. Veriyi yÃ¼kle

print("\n[1/5] Veri yÃ¼kleniyor...")

    (X_train, y_train), (X_val, y_val), (X_test, y_test), metadata = load_processed_dataset('processed_data')


# Masks oluÅŸtur (padding detection)

    masks_train = (X_train.sum(axis=-1) !=0).astype(np.float32)

    masks_val = (X_val.sum(axis=-1) !=0).astype(np.float32)


# PyTorch tensors

    train_dataset = TensorDataset(

        torch.FloatTensor(X_train),

        torch.LongTensor(y_train),

        torch.FloatTensor(masks_train)

    )

    val_dataset = TensorDataset(

        torch.FloatTensor(X_val),

        torch.LongTensor(y_val),

        torch.FloatTensor(masks_val)

    )


    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)

    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)


# 2. Model oluÅŸtur

print("\n[2/5] Model oluÅŸturuluyor...")

    model = SignLanguageTransformer(

input_dim=258,

d_model=256,

num_heads=8,

num_layers=6,

d_ff=1024,

num_classes=3,

dropout=0.15

    ).to(DEVICE)


print(f"âœ… Model parametreleri: {sum(p.numel() for p in model.parameters()):,}")


# 3. Optimizer & Loss

print("\n[3/5] Optimizer ve loss hazÄ±rlanÄ±yor...")

    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)

    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)


# 4. Training loop

print("\n[4/5] EÄŸitim baÅŸlÄ±yor...")

    best_val_acc =0

    patience_counter =0

EARLY_STOP_PATIENCE=30


for epoch inrange(1, NUM_EPOCHS+1):

print(f"\n{'='*60}")

print(f"Epoch {epoch}/{NUM_EPOCHS}")

print(f"{'='*60}")


# Train

        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)


# Validate

        val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)


# Scheduler step

        scheduler.step(val_loss)


# Logging

print(f"\nğŸ“Š Epoch {epoch} SonuÃ§larÄ±:")

print(f"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")

print(f"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%")


# Model checkpoint (en iyi modeli kaydet)

if val_acc > best_val_acc:

            best_val_acc = val_acc

            patience_counter =0


            checkpoint = {

'epoch': epoch,

'model_state_dict': model.state_dict(),

'optimizer_state_dict': optimizer.state_dict(),

'val_acc': val_acc,

'val_loss': val_loss

            }

            torch.save(checkpoint, 'checkpoints/best_model.pth')

print(f"  âœ… En iyi model kaydedildi! (Val Acc: {val_acc:.2f}%)")

else:

            patience_counter +=1


# Early stopping

if patience_counter >=EARLY_STOP_PATIENCE:

print(f"\nâ¹ï¸ Early stopping! {EARLY_STOP_PATIENCE} epoch'ta iyileÅŸme yok.")

break


# 5. Training tamamlandÄ±

print("\n[5/5] Training tamamlandÄ±!")

print(f"âœ… En iyi validation accuracy: {best_val_acc:.2f}%")


if__name__=="__main__":

    main()

```

**KullanÄ±m:**

```bash

cdTID-N

pythontrain.py

```

### 4.7 TensorBoard Ä°zleme

EÄŸitim sÃ¼recini gÃ¶rselleÅŸtirmek iÃ§in:

```python

from torch.utils.tensorboard import SummaryWriter


# Training script baÅŸÄ±nda:

writer = SummaryWriter('runs/transformer_experiment_1')


# Her epoch sonunda:

writer.add_scalar('Loss/train', train_loss, epoch)

writer.add_scalar('Loss/val', val_loss, epoch)

writer.add_scalar('Accuracy/train', train_acc, epoch)

writer.add_scalar('Accuracy/val', val_acc, epoch)

writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)


# Training bitince:

writer.close()

```

**TensorBoard baÅŸlatma:**

```bash

tensorboard--logdir=runs

```

---

## 5. DeÄŸerlendirme Metrikleri

### 5.1 Test Seti Ãœzerinde DeÄŸerlendirme

```python

# evaluate.py

import torch

import numpy as np

from sklearn.metrics import (

    accuracy_score, precision_score, recall_score, f1_score,

    classification_report, confusion_matrix

)

import matplotlib.pyplot as plt

import seaborn as sns


defevaluate_model(model, test_loader, device, class_names):

"""

    Model'i test seti Ã¼zerinde kapsamlÄ± deÄŸerlendir


    Returns:

        metrics: Dictionary of evaluation metrics

        predictions: Array of predictions

        targets: Array of ground truth

    """

    model.eval()


    all_preds = []

    all_targets = []

    all_probs = []


with torch.no_grad():

for data, targets, masks in tqdm(test_loader, desc='Testing'):

            data = data.to(device)

            targets = targets.to(device)

            masks = masks.to(device)


            outputs = model(data, masks)

            probs = torch.softmax(outputs, dim=1)

            _, predicted = outputs.max(1)


            all_preds.extend(predicted.cpu().numpy())

            all_targets.extend(targets.cpu().numpy())

            all_probs.extend(probs.cpu().numpy())


    all_preds = np.array(all_preds)

    all_targets = np.array(all_targets)

    all_probs = np.array(all_probs)


# Metrikleri hesapla

    metrics = {

'accuracy': accuracy_score(all_targets, all_preds),

'precision_macro': precision_score(all_targets, all_preds, average='macro'),

'recall_macro': recall_score(all_targets, all_preds, average='macro'),

'f1_macro': f1_score(all_targets, all_preds, average='macro'),

'precision_per_class': precision_score(all_targets, all_preds, average=None),

'recall_per_class': recall_score(all_targets, all_preds, average=None),

'f1_per_class': f1_score(all_targets, all_preds, average=None)

    }


return metrics, all_preds, all_targets, all_probs

```

### 5.2 Metrik AÃ§Ä±klamalarÄ±

#### 5.2.1 Accuracy (DoÄŸruluk)

```

Accuracy = (DoÄŸru Tahminler) / (Toplam Tahminler)

```

**Ã–rnek:**

- 34 test Ã¶rneÄŸinden 30'u doÄŸru â†’ Accuracy = 30/34 = 88.2%

**KÄ±sÄ±tlamalar:**

- Dengesiz sÄ±nÄ±flar varsa yanÄ±ltÄ±cÄ± olabilir
- Ã–rnek: %90 abla, %10 diÄŸerleri â†’ Hep "abla" tahmin et â†’ %90 accuracy!

#### 5.2.2 Precision (Kesinlik)

```

Precision = True Positives / (True Positives + False Positives)

```

**AnlamÄ±:** Model "abla" dediÄŸinde ne kadar haklÄ±?

**Ã–rnek (abla sÄ±nÄ±fÄ± iÃ§in):**

- Model 12 kez "abla" dedi
- Bunlardan 10'u gerÃ§ekten abla idi
- Precision = 10/12 = 0.833

#### 5.2.3 Recall (DuyarlÄ±lÄ±k / Sensitivity)

```

Recall = True Positives / (True Positives + False Negatives)

```

**AnlamÄ±:** GerÃ§ek "abla" Ã¶rneklerinin kaÃ§Ä±nÄ± bulduk?

**Ã–rnek (abla sÄ±nÄ±fÄ± iÃ§in):**

- Test setinde 11 tane abla var
- Model bunlardan 10'unu buldu
- Recall = 10/11 = 0.909

#### 5.2.4 F1-Score

```

F1 = 2 * (Precision * Recall) / (Precision + Recall)

```

**AnlamÄ±:** Precision ve Recall'Ä±n harmonik ortalamasÄ±

**Ã–rnek:**

- Precision = 0.833, Recall = 0.909
- F1 = 2 * (0.833 * 0.909) / (0.833 + 0.909) = 0.870

### 5.3 Classification Report

```python

defprint_classification_report(targets, predictions, class_names):

"""

    SÄ±nÄ±f bazÄ±nda detaylÄ± rapor

    """

    report = classification_report(

        targets, predictions,

target_names=class_names,

digits=4

    )


print("\nğŸ“‹ Classification Report:")

print("="*60)

print(report)


# JSON formatÄ±nda kaydet

    report_dict = classification_report(

        targets, predictions,

target_names=class_names,

output_dict=True

    )


withopen('evaluation_report.json', 'w') as f:

        json.dump(report_dict, f, indent=4)


print("âœ… Rapor kaydedildi: evaluation_report.json")

```

**Ã–rnek Ã‡Ä±ktÄ±:**

```

              precision    recall  f1-score   support


        abla     0.8333    0.9091    0.8696        11

       acele     0.9000    0.8182    0.8571        11

     acikmak     0.9167    0.9167    0.9167        12


    accuracy                         0.8824        34

   macro avg     0.8833    0.8813    0.8812        34

weighted avg     0.8840    0.8824    0.8820        34

```

### 5.4 Confusion Matrix

Hangi sÄ±nÄ±flarÄ±n birbirine karÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶sterir:

```python

defplot_confusion_matrix(targets, predictions, class_names, save_path='confusion_matrix.png'):

"""

    Confusion matrix gÃ¶rselleÅŸtirmesi

    """

    cm = confusion_matrix(targets, predictions)

    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))


# Raw counts

    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 

xticklabels=class_names, yticklabels=class_names, ax=ax1)

    ax1.set_title('Confusion Matrix (Raw Counts)')

    ax1.set_ylabel('True Label')

    ax1.set_xlabel('Predicted Label')


# Normalized

    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',

xticklabels=class_names, yticklabels=class_names, ax=ax2)

    ax2.set_title('Confusion Matrix (Normalized)')

    ax2.set_ylabel('True Label')

    ax2.set_xlabel('Predicted Label')


    plt.tight_layout()

    plt.savefig(save_path, dpi=300, bbox_inches='tight')

print(f"âœ… Confusion matrix kaydedildi: {save_path}")

    plt.show()

```

**Ã–rnek Confusion Matrix:**

```

           abla  acele  acikmak

abla         10      1        0

acele         1      9        1

acikmak       0      1       11

```

**Yorumlama:**

- Diagonal (kÃ¶ÅŸegen) yÃ¼ksek = Ä°yi!
- abla â†’ acele: 1 kez karÄ±ÅŸtÄ±rÄ±ldÄ±
- acele â†’ acikmak: 1 kez karÄ±ÅŸtÄ±rÄ±ldÄ±

### 5.5 Per-Class Visualizations

SÄ±nÄ±f bazlÄ± performans grafikleri:

```python

defplot_per_class_metrics(metrics, class_names, save_path='per_class_metrics.png'):

"""

    Her sÄ±nÄ±f iÃ§in precision, recall, f1-score grafiÄŸi

    """

    x = np.arange(len(class_names))

    width =0.25


    fig, ax = plt.subplots(figsize=(12, 6))


    ax.bar(x - width, metrics['precision_per_class'], width, label='Precision', alpha=0.8)

    ax.bar(x, metrics['recall_per_class'], width, label='Recall', alpha=0.8)

    ax.bar(x + width, metrics['f1_per_class'], width, label='F1-Score', alpha=0.8)


    ax.set_ylabel('Score')

    ax.set_title('Per-Class Performance Metrics')

    ax.set_xticks(x)

    ax.set_xticklabels(class_names)

    ax.legend()

    ax.grid(axis='y', alpha=0.3)

    ax.set_ylim([0, 1.0])


    plt.tight_layout()

    plt.savefig(save_path, dpi=300, bbox_inches='tight')

print(f"âœ… Per-class metrics grafiÄŸi kaydedildi: {save_path}")

    plt.show()

```

### 5.6 Confidence Distribution

Model ne kadar gÃ¼venli tahmin yapÄ±yor?

```python

defplot_confidence_distribution(probs, targets, save_path='confidence_dist.png'):

"""

    Tahmin gÃ¼ven daÄŸÄ±lÄ±mÄ±

    """

# Her Ã¶rnek iÃ§in maximum probability

    max_probs = probs.max(axis=1)


# DoÄŸru ve yanlÄ±ÅŸ tahminler iÃ§in ayrÄ± ayrÄ±

    predictions = probs.argmax(axis=1)

    correct_mask = (predictions == targets)


    correct_probs = max_probs[correct_mask]

    incorrect_probs = max_probs[~correct_mask]


    fig, ax = plt.subplots(figsize=(10, 6))


    ax.hist(correct_probs, bins=20, alpha=0.7, label='Correct Predictions', color='green', range=(0, 1))

    ax.hist(incorrect_probs, bins=20, alpha=0.7, label='Incorrect Predictions', color='red', range=(0, 1))


    ax.set_xlabel('Confidence (Max Probability)')

    ax.set_ylabel('Count')

    ax.set_title('Prediction Confidence Distribution')

    ax.legend()

    ax.grid(axis='y', alpha=0.3)


    plt.tight_layout()

    plt.savefig(save_path, dpi=300, bbox_inches='tight')

print(f"âœ… Confidence distribution grafiÄŸi kaydedildi: {save_path}")

    plt.show()


# Ä°statistikler

print("\nğŸ“Š Confidence Ä°statistikleri:")

print(f"  DoÄŸru tahminler - Ortalama gÃ¼ven: {correct_probs.mean():.4f} Â± {correct_probs.std():.4f}")

print(f"  YanlÄ±ÅŸ tahminler - Ortalama gÃ¼ven: {incorrect_probs.mean():.4f} Â± {incorrect_probs.std():.4f}")

```

### 5.7 Tam Evaluation Script

```python

# evaluate.py

defmain():

DEVICE= torch.device('cuda'if torch.cuda.is_available() else'cpu')


print("ğŸ” Model DeÄŸerlendirme BaÅŸlÄ±yor")

print("="*60)


# 1. Test verisini yÃ¼kle

print("\n[1/6] Test verisi yÃ¼kleniyor...")

    (X_train, y_train), (X_val, y_val), (X_test, y_test), metadata = load_processed_dataset('processed_data')

    class_names = metadata['class_names']


    masks_test = (X_test.sum(axis=-1) !=0).astype(np.float32)

    test_dataset = TensorDataset(

        torch.FloatTensor(X_test),

        torch.LongTensor(y_test),

        torch.FloatTensor(masks_test)

    )

    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)


# 2. Model yÃ¼kle

print("\n[2/6] Model yÃ¼kleniyor...")

    model = SignLanguageTransformer(

input_dim=258, d_model=256, num_heads=8,

num_layers=6, d_ff=1024, num_classes=3

    ).to(DEVICE)


    checkpoint = torch.load('checkpoints/best_model.pth', map_location=DEVICE)

    model.load_state_dict(checkpoint['model_state_dict'])

print(f"âœ… Model yÃ¼klendi (Epoch {checkpoint['epoch']}, Val Acc: {checkpoint['val_acc']:.2f}%)")


# 3. Evaluate

print("\n[3/6] Test seti Ã¼zerinde deÄŸerlendirme yapÄ±lÄ±yor...")

    metrics, predictions, targets, probs = evaluate_model(model, test_loader, DEVICE, class_names)


# 4. Metrikleri yazdÄ±r

print("\n[4/6] Metrikler hesaplandÄ±:")

print(f"  Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")

print(f"  Precision: {metrics['precision_macro']:.4f}")

print(f"  Recall:    {metrics['recall_macro']:.4f}")

print(f"  F1-Score:  {metrics['f1_macro']:.4f}")


# 5. GÃ¶rselleÅŸtirmeler

print("\n[5/6] GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...")

    print_classification_report(targets, predictions, class_names)

    plot_confusion_matrix(targets, predictions, class_names, 'results/confusion_matrix.png')

    plot_per_class_metrics(metrics, class_names, 'results/per_class_metrics.png')

    plot_confidence_distribution(probs, targets, 'results/confidence_distribution.png')


# 6. SonuÃ§larÄ± kaydet

print("\n[6/6] SonuÃ§lar kaydediliyor...")

    results = {

'test_accuracy': float(metrics['accuracy']),

'test_precision': float(metrics['precision_macro']),

'test_recall': float(metrics['recall_macro']),

'test_f1': float(metrics['f1_macro']),

'per_class_metrics': {

            class_names[i]: {

'precision': float(metrics['precision_per_class'][i]),

'recall': float(metrics['recall_per_class'][i]),

'f1': float(metrics['f1_per_class'][i])

            }

for i inrange(len(class_names))

        }

    }


withopen('results/evaluation_results.json', 'w') as f:

        json.dump(results, f, indent=4)


print("\nâœ… DeÄŸerlendirme tamamlandÄ±!")

print(f"ğŸ“ SonuÃ§lar 'results/' klasÃ¶rÃ¼ne kaydedildi")


if__name__=="__main__":

    main()

```

**KullanÄ±m:**

```bash

cdTID-N

pythonevaluate.py

```

---

## 6. Gerekli KÃ¼tÃ¼phaneler

### 6.1 requirements.txt

```txt

# Deep Learning Framework

torch>=2.0.0

torchvision>=0.15.0

torchaudio>=2.0.0


# Alternative: TensorFlow

# tensorflow>=2.12.0


# Data Processing

numpy>=1.24.0

pandas>=2.0.0

scipy>=1.10.0


# Computer Vision

opencv-python>=4.7.0

mediapipe>=0.10.0


# Machine Learning Utilities

scikit-learn>=1.2.0


# Visualization

matplotlib>=3.7.0

seaborn>=0.12.0

tensorboard>=2.12.0


# Progress Bars

tqdm>=4.65.0


# Utilities

pillow>=9.5.0

pyyaml>=6.0

```

### 6.2 Kurulum

```bash

# Virtual environment oluÅŸtur

python3-mvenvvenv

sourcevenv/bin/activate# Linux/Mac

# veya

venv\Scripts\activate# Windows


# KÃ¼tÃ¼phaneleri kur

pipinstall-rrequirements.txt


# GPU desteÄŸi iÃ§in (CUDA 11.8)

pipinstalltorchtorchvisiontorchaudio--index-urlhttps://download.pytorch.org/whl/cu118

```

### 6.3 Sistem Gereksinimleri

**Minimum:**

- CPU: 4 cores
- RAM: 8 GB
- Disk: 5 GB boÅŸ alan
- GPU: Opsiyonel (CPU ile de Ã§alÄ±ÅŸÄ±r)

**Ã–nerilen:**

- CPU: 8+ cores
- RAM: 16 GB
- Disk: 20 GB boÅŸ alan
- GPU: NVIDIA GTX 1660 veya Ã¼zeri (6GB+ VRAM)
- CUDA: 11.8 veya Ã¼zeri

---

## 7. Proje Dosya YapÄ±sÄ±

```

TID-N/

â”œâ”€â”€ README.md                    # Bu dosya

â”œâ”€â”€ requirements.txt             # KÃ¼tÃ¼phane baÄŸÄ±mlÄ±lÄ±klarÄ±

â”œâ”€â”€ config.py                    # KonfigÃ¼rasyon parametreleri

â”‚

â”œâ”€â”€ prepare_data.py              # Veri hazÄ±rlama scripti

â”œâ”€â”€ train.py                     # EÄŸitim scripti

â”œâ”€â”€ evaluate.py                  # DeÄŸerlendirme scripti

â”œâ”€â”€ infer_videos.py              # Inference scripti

â”‚

â”œâ”€â”€ models/

â”‚   â”œâ”€â”€ __init__.py

â”‚   â”œâ”€â”€ transformer.py           # Transformer model tanÄ±mÄ±

â”‚   â””â”€â”€ utils.py                 # Model yardÄ±mcÄ± fonksiyonlarÄ±

â”‚

â”œâ”€â”€ utils/

â”‚   â”œâ”€â”€ __init__.py

â”‚   â”œâ”€â”€ data_loader.py           # Veri yÃ¼kleme fonksiyonlarÄ±

â”‚   â”œâ”€â”€ augmentation.py          # Data augmentation

â”‚   â””â”€â”€ visualization.py         # GÃ¶rselleÅŸtirme fonksiyonlarÄ±

â”‚

â”œâ”€â”€ videos/                      # Ham keypoint verileri

â”‚   â”œâ”€â”€ abla/

â”‚   â”œâ”€â”€ acele/

â”‚   â””â”€â”€ acikmak/

â”‚

â”œâ”€â”€ processed_data/              # Ä°ÅŸlenmiÅŸ veri seti

â”‚   â”œâ”€â”€ X_train.npy

â”‚   â”œâ”€â”€ y_train.npy

â”‚   â”œâ”€â”€ X_val.npy

â”‚   â”œâ”€â”€ y_val.npy

â”‚   â”œâ”€â”€ X_test.npy

â”‚   â”œâ”€â”€ y_test.npy

â”‚   â””â”€â”€ metadata.pkl

â”‚

â”œâ”€â”€ checkpoints/                 # Model checkpoint'leri

â”‚   â”œâ”€â”€ best_model.pth

â”‚   â””â”€â”€ ...

â”‚

â”œâ”€â”€ results/                     # DeÄŸerlendirme sonuÃ§larÄ±

â”‚   â”œâ”€â”€ confusion_matrix.png

â”‚   â”œâ”€â”€ per_class_metrics.png

â”‚   â”œâ”€â”€ confidence_distribution.png

â”‚   â””â”€â”€ evaluation_results.json

â”‚

â””â”€â”€ runs/                        # TensorBoard loglarÄ±

    â””â”€â”€ transformer_experiment_1/

```

---

## 8. KullanÄ±m TalimatlarÄ±

### 8.1 AdÄ±m AdÄ±m Ã‡alÄ±ÅŸtÄ±rma

```bash

# 1. Repo'yu klonla

cdTID-N


# 2. Virtual environment oluÅŸtur

python3-mvenvvenv

sourcevenv/bin/activate


# 3. BaÄŸÄ±mlÄ±lÄ±klarÄ± kur

pipinstall-rrequirements.txt


# 4. Veriyi hazÄ±rla

pythonprepare_data.py

# Ã‡Ä±ktÄ±: processed_data/ klasÃ¶rÃ¼ oluÅŸturulur


# 5. Model eÄŸit

pythontrain.py

# Ã‡Ä±ktÄ±: checkpoints/best_model.pth kaydedilir


# 6. Modeli deÄŸerlendir

pythonevaluate.py

# Ã‡Ä±ktÄ±: results/ klasÃ¶rÃ¼nde gÃ¶rselleÅŸtirmeler


# 7. TensorBoard ile izle (opsiyonel)

tensorboard--logdir=runs

```

### 8.2 Hiperparametre Tuning

`config.py` veya `train.py` iÃ§inde ayarlanabilir:

```python

# Model boyutu

D_MODEL=256# 128, 256, 512

NUM_HEADS=8# 4, 8, 16

NUM_LAYERS=6# 3, 6, 9, 12

D_FF=1024# 512, 1024, 2048


# Training

BATCH_SIZE=32# 16, 32, 64

LEARNING_RATE=1e-4# 5e-5, 1e-4, 5e-4

DROPOUT=0.15# 0.1, 0.15, 0.2


# Data

SEQ_LEN=60# 30, 60, 90

```

---

## 9. Sonraki AdÄ±mlar ve Ä°yileÅŸtirmeler

### 9.1 Model Ä°yileÅŸtirmeleri

1.**Daha Fazla Kelime:** Ä°lk 3'ten tÃ¼m 226 kelimeye geniÅŸletin

2.**Ensemble:** Birden fazla model'in tahminlerini birleÅŸtirin

3.**Temporal Attention Visualization:** Hangi frame'lere odaklanÄ±yor?

4.**Multi-Modal:** Video + Optik AkÄ±ÅŸ + YÃ¼z ifadeleri

### 9.2 Deployment

1.**ONNX Export:** Model'i ONNX formatÄ±na Ã§evirerek platform baÄŸÄ±msÄ±z hale getirin

2.**Quantization:** Model boyutunu kÃ¼Ã§Ã¼ltÃ¼n (INT8)

3.**Real-Time Inference:** Webcam Ã¼zerinden gerÃ§ek zamanlÄ± tahmin

4.**Web/Mobile App:** Flask/FastAPI ile API oluÅŸturun

### 9.3 Veri ArtÄ±rma

1.**Video Augmentation:** HÄ±z deÄŸiÅŸtirme, perspektif dÃ¶nÃ¼ÅŸÃ¼mÃ¼

2.**Mixup/CutMix:** FarklÄ± Ã¶rnekleri karÄ±ÅŸtÄ±rÄ±n

3.**Synthetic Data:** GAN ile sentetik iÅŸaret dili videolarÄ±

---

## 10. Kaynaklar ve Referanslar

### 10.1 Transformer Makaleleri

1.**Attention Is All You Need** (Vaswani et al., 2017)

- Orijinal Transformer makalesi
- https://arxiv.org/abs/1706.03762

2.**Video Action Recognition Transformer** (VoVNet, 2021)

- Video sÄ±nÄ±flandÄ±rma iÃ§in Transformer
- https://arxiv.org/abs/2103.15691

3.**TimeSformer** (Facebook AI, 2021)

- Divided Space-Time Attention
- https://arxiv.org/abs/2102.05095

### 10.2 Ä°ÅŸaret Dili TanÄ±ma

1.**Sign Language Recognition Survey** (2020)

- Ä°ÅŸaret dili tanÄ±ma teknikleri
- https://arxiv.org/abs/2008.09918

2.**MediaPipe Holistic** (Google, 2020)

- Keypoint extraction
- https://google.github.io/mediapipe/

### 10.3 FaydalÄ± Linkler

- PyTorch Transformer Tutorial: https://pytorch.org/tutorials/beginner/transformer_tutorial.html
- Hugging Face Transformers: https://huggingface.co/docs/transformers/
- Papers with Code (Sign Language): https://paperswithcode.com/task/sign-language-recognition

---

## 11. Lisans ve TeÅŸekkÃ¼rler

Bu proje, TÃ¼rk Ä°ÅŸaret Dili (TÄ°D) araÅŸtÄ±rmalarÄ± iÃ§in geliÅŸtirilmiÅŸtir.

**Veri Seti:** TÄ°D (Turkish Sign Language) Dataset

**GeliÅŸtirici:** [Ä°sminiz]

**Tarih:** Ekim 2025

---

## 12. Ä°letiÅŸim

SorularÄ±nÄ±z veya katkÄ±larÄ±nÄ±z iÃ§in:

- Email: [email@example.com]
- GitHub Issues: [repo-link]

**Mutlu Kodlamalar! ğŸš€ğŸ¤Ÿ**
