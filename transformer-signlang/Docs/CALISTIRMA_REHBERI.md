# ğŸš€ TRANSFORMER SIGN LANGUAGE - Ã‡ALIÅTIRMA REHBERÄ°

## âœ… Ã–n KoÅŸullar (TAMAMLANDI)
- [x] Conda environment "transformers" kuruldu
- [x] Python 3.10+ yÃ¼klÃ¼
- [x] TÃ¼m dependencies kuruldu (`requirements.txt`)
- [x] Veri setleri mevcut (56k train, 8.8k val, 7.4k test)

---

## ğŸ“‹ ADIM ADIM Ã‡ALIÅTIRMA

### ğŸ”§ 0. Environment Aktivasyonu

```bash
# Terminal'de Ã§alÄ±ÅŸtÄ±r:
conda activate transformers
cd /Users/siyaksares/Developer/GitHub/klassifier-sign-language/transformer-signlang
```

**Kontrol:**
```bash
python --version  # Python 3.10+ olmalÄ±
which python      # transformers env'Ä±ndaki python olmalÄ±
```

---

### ğŸ“Š 1. VERÄ° HAZIRLAMA AÅAMASI

#### AdÄ±m 1.1: Video SeÃ§imi
**Script:** `scripts/01_select_videos.py`

```bash
python scripts/01_select_videos.py
```

**Ne yapar:**
- Train/Val/Test setlerinden 3 kelimeye (acele, acikmak, agac) ait videolarÄ± seÃ§er
- ClassId 1, 2, 5'e karÅŸÄ±lÄ±k gelen videolarÄ± filtreler
- CSV dosyalarÄ± oluÅŸturur

**Beklenen SÃ¼re:** ~1-2 dakika

**Ã‡Ä±ktÄ±lar:**
```
data/selected_videos_train.csv
data/selected_videos_val.csv
data/selected_videos_test.csv
```

**DoÄŸrulama:**
```bash
# KaÃ§ video seÃ§ildi?
wc -l data/selected_videos_*.csv

# Ä°Ã§eriÄŸe bakÄ±n:
head -5 data/selected_videos_train.csv
```

**Beklenen SonuÃ§:**
- Her CSV'de video_path, class_id, path, split kolonlarÄ±
- Train: 373 video (ClassId 1,2,5)
- Val: 59 video (ClassId 1,2,5)
- Test: 50 video (ClassId 1,2,5)

**âš ï¸ Ã–NEMLI NOT:**
Script otomatik olarak doÄŸru label dosyalarÄ±nÄ± kullanÄ±r:
- Validation: `ground_truth 2.csv` (4,418 satÄ±r)
- Test: `ground_truth.csv` (3,742 satÄ±r)

---

#### AdÄ±m 1.2: Keypoint Extraction
**Script:** `scripts/02_extract_keypoints.py`

```bash
python scripts/02_extract_keypoints.py
```

**Ne yapar:**
- Her videodan MediaPipe ile keypoint'leri Ã§Ä±karÄ±r
- Pose (99D) + Face (33D) + Left Hand (63D) + Right Hand (63D) = 258D
- Her video iÃ§in `.npy` dosyasÄ± oluÅŸturur

**âš ï¸ Ã–NEMLÄ°:**
- **Bu adÄ±m EN UZUN sÃ¼ren adÄ±mdÄ±r!**
- Video sayÄ±sÄ±na gÃ¶re **30-90 dakika** sÃ¼rebilir
- Progress bar ile ilerlemeyi takip edebilirsiniz

**Beklenen SÃ¼re:** ~30-90 dakika (video sayÄ±sÄ±na gÃ¶re)

**Ã‡Ä±ktÄ±lar:**
```
data/keypoints/
â”œâ”€â”€ signer0_sample16.npy
â”œâ”€â”€ signer0_sample25.npy
â”œâ”€â”€ signer0_sample29.npy
â””â”€â”€ ... (482 dosya - tÃ¼m train/val/test videolarÄ±)
```

**DoÄŸrulama:**
```bash
# KaÃ§ keypoint dosyasÄ± oluÅŸturuldu?
ls data/keypoints/*.npy | wc -l
# Beklenen: 482

# Bir dosyanÄ±n boyutunu kontrol et:
python -c "import numpy as np; data = np.load('data/keypoints/signer0_sample16.npy'); print(f'Shape: {data.shape}')"
# Beklenen: Shape: (frame_count, 258)
```

**Ä°pucu:**
- Script progress bar gÃ¶sterir
- Her 10 videoda bir otomatik kayÄ±t yapar
- Kesintide kaldÄ±ÄŸÄ± yerden devam eder

---

#### AdÄ±m 1.3: Normalization & Padding
**Script:** `scripts/03_normalize_data.py`

```bash
python scripts/03_normalize_data.py
```

**Ne yapar:**
- Keypoint'leri Z-score normalization ile normalize eder
- Scaler'Ä± **sadece train data**'da fit eder (data leakage Ã¶nlenir)
- TÃ¼m sequence'leri aynÄ± uzunluÄŸa getirir (padding/truncate)
- MAX_SEQ_LENGTH: 95th percentile'a gÃ¶re belirlenir

**Beklenen SÃ¼re:** ~2-5 dakika

**Ã‡Ä±ktÄ±lar:**
```
data/processed/
â”œâ”€â”€ X_train.npy        # (N_train, seq_len, 258)
â”œâ”€â”€ y_train.npy        # (N_train,)
â”œâ”€â”€ train_ids.npy      # (N_train,)
â”œâ”€â”€ X_val.npy          # (N_val, seq_len, 258)
â”œâ”€â”€ y_val.npy          # (N_val,)
â”œâ”€â”€ val_ids.npy        # (N_val,)
â”œâ”€â”€ X_test.npy         # (N_test, seq_len, 258)
â”œâ”€â”€ y_test.npy         # (N_test,)
â”œâ”€â”€ test_ids.npy       # (N_test,)
â””â”€â”€ metadata.json

data/scaler.pkl         # StandardScaler (train'de fit)
```

**DoÄŸrulama:**
```bash
# Shape'leri kontrol et:
python -c "
import numpy as np
print('Train:', np.load('data/processed/X_train.npy').shape)
print('Val:  ', np.load('data/processed/X_val.npy').shape)
print('Test: ', np.load('data/processed/X_test.npy').shape)
"

# Metadata'yÄ± incele:
cat data/processed/metadata.json
```

**Beklenen SonuÃ§:**
```
Train: (N_train, seq_len, 258)
Val:   (N_val, seq_len, 258)
Test:  (N_test, seq_len, 258)

seq_len: ~150-200 frame (95th percentile)
```

---

### ğŸ“ 2. MODEL EÄÄ°TÄ°MÄ°

#### AdÄ±m 2: Training
**Script:** `train.py`

```bash
python train.py
```

**Ne yapar:**
- Transformer modelini eÄŸitir
- Validation accuracy'ye gÃ¶re best model'i kaydeder
- Early stopping ile gereksiz eÄŸitimi Ã¶nler
- Training history'yi JSON olarak kaydeder

**Model Ã–zellikleri:**
- **Architecture:** 6-layer Transformer Encoder
- **d_model:** 256
- **Attention heads:** 8
- **Feedforward dim:** 1024
- **Optimizer:** AdamW (lr=1e-4)
- **Scheduler:** Cosine Annealing with Warmup (10 epochs)
- **Loss:** Label Smoothing Cross-Entropy (Îµ=0.1)
- **Early Stopping:** 10 epoch patience

**Beklenen SÃ¼re:**
- **GPU (CUDA):** ~30-60 dakika
- **CPU (M1/M2/M3):** ~90-150 dakika
- **CPU (Intel):** ~120-240 dakika

**âš ï¸ Ã–NEMLÄ°:**
- Progress bar gÃ¶sterir (epoch/batch tracking)
- Her epoch sonunda val accuracy/loss yazdÄ±rÄ±r
- Best model otomatik kaydedilir
- CUDA varsa otomatik GPU kullanÄ±r

**Ã‡Ä±ktÄ±lar:**
```
checkpoints/
â”œâ”€â”€ best_model.pth     # En iyi val accuracy modeli
â””â”€â”€ last_model.pth     # Son checkpoint

logs/
â””â”€â”€ training_history.json  # Loss/accuracy history
```

**Monitoring:**
```bash
# Training sÄ±rasÄ±nda baÅŸka bir terminal'de:
tail -f logs/training.log  # (eÄŸer log dosyasÄ± oluÅŸturuluyorsa)

# Training sonrasÄ± history'yi incele:
cat logs/training_history.json | python -m json.tool | head -30
```

**DoÄŸrulama:**
```bash
# Checkpoint'leri kontrol et:
ls -lh checkpoints/

# Best model'in epoch/accuracy bilgisi:
python -c "
import torch
ckpt = torch.load('checkpoints/best_model.pth', map_location='cpu')
print(f'Epoch: {ckpt[\"epoch\"]}')
print(f'Val Accuracy: {ckpt[\"val_acc\"]:.4f}')
print(f'Val F1: {ckpt[\"val_f1\"]:.4f}')
"
```

**Beklenen Performans (Ä°lk 3 Kelime):**
- **Accuracy:** %70-85
- **F1-Score:** %68-83
- **Loss (final):** 0.3-0.6

#### ğŸ”„ Checkpoint Resume (KaldÄ±ÄŸÄ± Yerden Devam)

**NEW!** EÄŸitim yarÄ±da kesildiyse kaldÄ±ÄŸÄ± yerden devam edebilirsiniz:

**KullanÄ±m SenaryolarÄ±:**

**1. Normal EÄŸitim (SÄ±fÄ±rdan):**
```bash
python train.py
```

**2. Last Checkpoint'ten Devam:**
```bash
# EÄŸitim kesildiyse (Ctrl+C, elektrik kesintisi, vb.)
python train.py --resume checkpoints/last_model.pth
```

**3. Best Model'den Devam (Fine-tuning):**
```bash
# En iyi model'den devam et
python train.py --resume-from-best
```

**4. Spesifik Checkpoint'ten Devam:**
```bash
python train.py --resume checkpoints/epoch_50.pth
```

**Resume Ã–zelliÄŸi DetaylarÄ±:**

| Ã–zellik | AÃ§Ä±klama |
|---------|----------|
| âœ… Model Weights | Tam olarak kaldÄ±ÄŸÄ± yerden |
| âœ… Optimizer State | Momentum ve variance korunur |
| âœ… LR Scheduler | Learning rate doÄŸru pozisyondan |
| âœ… Best Accuracy | En iyi skor takibi devam eder |
| âœ… Training History | Grafikler kopuksuz devam eder |
| âœ… Early Stopping | Patience counter korunur |

**Console Output Ã–rneÄŸi:**
```
ğŸ“‚ Loading checkpoint from checkpoints/last_model.pth...
   âœ… Model weights loaded
   âœ… Optimizer state loaded
   âœ… Scheduler state loaded
   ğŸ“Š Resuming from epoch 26
   ğŸ“ˆ Best val accuracy: 0.8542
   ğŸ“ˆ Best val F1: 0.8401
   ğŸ“œ Training history restored (25 epochs)
   â³ Early stopping patience counter: 3/15

âœ… Successfully loaded checkpoint!
   Training will resume from epoch 26

ğŸ”„ RESUMING TRAINING from Epoch 26
```

**FaydalarÄ±:**
- ğŸ”´ **Elektrik Kesintisi:** EÄŸitim kaybÄ± yok
- ğŸ¯ **GPU Timeout:** Uzun eÄŸitimleri bÃ¶lebilirsin
- âš¡ **Hiperparametre DeÄŸiÅŸikliÄŸi:** Ä°stediÄŸin noktadan farklÄ± LR ile devam
- ğŸ’¾ **Disk Tasarrufu:** Her epoch'u kaydetmeye gerek yok

**Checkpoint DosyasÄ± Ä°Ã§eriÄŸi:**
```python
checkpoint = {
    'epoch': 25,                         # Hangi epoch'ta
    'model_state_dict': ...,            # Model aÄŸÄ±rlÄ±klarÄ±
    'optimizer_state_dict': ...,        # AdamW momentum/variance
    'scheduler_state_dict': ...,        # LR scheduler position
    'val_acc': 0.8542,                  # En iyi val accuracy
    'val_f1': 0.8401,                   # En iyi val F1
    'history': {...},                   # Training curves
    'patience_counter': 3,              # Early stopping counter
    'config': {...}                     # TÃ¼m hiperparametreler
}
```

**Ã–nemli Notlar:**
- âš ï¸ Checkpoint ve yeni kod aynÄ± model architecture'a sahip olmalÄ±
- âš ï¸ Resume edilirken config.NUM_EPOCHS yeterince bÃ¼yÃ¼k olmalÄ±
- âœ… Hata durumunda otomatik sÄ±fÄ±rdan baÅŸlar (gÃ¼venli)

---

### ğŸ“Š 3. MODEL DEÄERLENDÄ°RME

#### AdÄ±m 3: Evaluation
**Script:** `evaluate.py`

```bash
python evaluate.py
```

**Ne yapar:**
- Test setinde model performansÄ±nÄ± Ã¶lÃ§er
- Comprehensive metrics hesaplar
- Confusion matrix oluÅŸturur
- Per-class performance analizi
- Visualization'lar oluÅŸturur

**Beklenen SÃ¼re:** ~2-5 dakika

**Ã‡Ä±ktÄ±lar:**
```
results/
â”œâ”€â”€ evaluation_report.json              # TÃ¼m metrics
â”œâ”€â”€ confusion_matrix_raw.csv            # Raw confusion matrix
â”œâ”€â”€ confusion_matrix_normalized.csv     # Normalized CM
â”œâ”€â”€ confusion_matrix_raw.png            # Raw CM heatmap
â”œâ”€â”€ confusion_matrix_normalized.png     # Normalized CM heatmap
â”œâ”€â”€ per_class_metrics.csv               # Per-class precision/recall/F1
â”œâ”€â”€ per_class_metrics.png               # Bar chart
â””â”€â”€ prediction_confidence.png           # Confidence distribution
```

**Metrics:**
- **Overall:** Accuracy, Precision (macro/weighted), Recall, F1-Score
- **Per-Class:** Her sÄ±nÄ±f iÃ§in ayrÄ± metrics
- **Confusion Matrix:** Raw counts ve normalized
- **Confidence Analysis:** DoÄŸru/yanlÄ±ÅŸ tahminlerin gÃ¼ven daÄŸÄ±lÄ±mÄ±

**DoÄŸrulama:**
```bash
# Results'larÄ± listele:
ls results/

# Evaluation report'u incele:
cat results/evaluation_report.json | python -m json.tool

# Per-class metrics:
cat results/per_class_metrics.csv
```

**Beklenen Ã‡Ä±ktÄ±:**
```json
{
    "overall": {
        "accuracy": 0.75-0.85,
        "precision_macro": 0.73-0.83,
        "recall_macro": 0.72-0.82,
        "f1_macro": 0.72-0.82
    },
    "per_class": {
        "acele": {"precision": 0.xx, "recall": 0.xx, "f1_score": 0.xx},
        "acikmak": {"precision": 0.xx, "recall": 0.xx, "f1_score": 0.xx},
        "agac": {"precision": 0.xx, "recall": 0.xx, "f1_score": 0.xx}
    }
}
```

---

### ğŸ¨ 4. ATTENTION VISUALIZATION

#### AdÄ±m 4: Attention GÃ¶rselleÅŸtirme
**Script:** `visualize_attention.py`

```bash
# Default: 5 sample
python visualize_attention.py

# Daha fazla sample:
python visualize_attention.py --num_samples 10
```

**Ne yapar:**
- Test setinden random Ã¶rnekler seÃ§er
- Her layer'Ä±n attention weights'lerini Ã§Ä±karÄ±r
- Multi-head attention'larÄ± gÃ¶rselleÅŸtirir
- Attention rollout (cumulative) hesaplar
- Layer/Head bazÄ±nda istatistikler

**Beklenen SÃ¼re:** ~3-10 dakika (sample sayÄ±sÄ±na gÃ¶re)

**Ã‡Ä±ktÄ±lar:**
```
results/attention/
â”œâ”€â”€ sample_0_layer_0_multihead.png     # Multi-head attention (2x4 grid)
â”œâ”€â”€ sample_0_layer_0_avg.png           # Average attention
â”œâ”€â”€ sample_0_layer_1_multihead.png
â”œâ”€â”€ sample_0_layer_1_avg.png
â”œâ”€â”€ ...
â”œâ”€â”€ sample_0_attention_rollout.png     # Cumulative attention
â”œâ”€â”€ sample_1_...                       # Ä°kinci sample
â”œâ”€â”€ ...
â”œâ”€â”€ layer_wise_attention_stats.png     # Layer statistics
â””â”€â”€ head_wise_attention_stats.png      # Head statistics
```

**Toplam Dosya:** ~47 dosya (5 samples Ã— 6 layers Ã— 2 + 5 rollout + 2 stats)

**DoÄŸrulama:**
```bash
# KaÃ§ gÃ¶rselleÅŸtirme oluÅŸturuldu?
ls results/attention/*.png | wc -l

# Preview (macOS):
open results/attention/sample_0_attention_rollout.png
open results/attention/layer_wise_attention_stats.png
```

**Insight'lar:**
- **Temporal patterns:** Hangi frame'lere odaklanÄ±lÄ±yor?
- **Head specialization:** Her head farklÄ± pattern mÄ±?
- **Layer hierarchy:** Alt layer local, Ã¼st layer global mÄ±?

---

## âœ… TAMAMLANAN CHECKLIST

Pipeline'Ä± tamamladÄ±ktan sonra:

```bash
# 1. TÃ¼m data dosyalarÄ± oluÅŸturuldu mu?
ls data/selected_videos_*.csv
ls data/keypoints/*.npy | wc -l
ls data/processed/*.npy

# 2. Model eÄŸitildi mi?
ls checkpoints/

# 3. Evaluation tamamlandÄ± mÄ±?
ls results/*.png
ls results/*.csv
ls results/*.json

# 4. Attention visualization tamamlandÄ± mÄ±?
ls results/attention/*.png | wc -l
```

---

## ğŸ“Š BEKLENEN SONUÃ‡LAR Ã–ZET

### Performans Metrikleri (Ä°lk 3 Kelime)
```
Accuracy:          75-85%
Precision (macro): 73-83%
Recall (macro):    72-82%
F1-Score (macro):  72-82%
```

### Dosya SayÄ±larÄ±
```
Keypoints:         ~500-1000 .npy dosyasÄ±
Processed data:    9 .npy + 1 .pkl + 1 .json
Checkpoints:       2 .pth dosyasÄ±
Evaluation:        8 dosya (4 CSV + 4 PNG)
Attention:         ~47 PNG dosyasÄ±
```

### Toplam SÃ¼re
```
Veri HazÄ±rlama:    ~35-100 dakika
Model EÄŸitimi:     ~30-240 dakika (GPU/CPU)
Evaluation:        ~2-5 dakika
Visualization:     ~3-10 dakika
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOPLAM:            ~70-355 dakika (1-6 saat)
```

---

## ğŸ”§ SORUN GÄ°DERME

### Hata: "No module named 'torch'"
```bash
# Environment'Ä± kontrol edin:
conda activate transformers
pip list | grep torch
```

### Hata: "FileNotFoundError: Data/Train Data/train"
```bash
# config.py'daki veri yollarÄ±nÄ± kontrol edin
# BASE_DATA_DIR doÄŸru mu?
```

### Hata: "CUDA out of memory"
```bash
# config.py'da BATCH_SIZE'Ä± kÃ¼Ã§Ã¼ltÃ¼n:
BATCH_SIZE = 16  # veya 8
```

### Training Ã§ok yavaÅŸ
```bash
# GPU kullanÄ±ldÄ±ÄŸÄ±nÄ± kontrol edin:
python -c "import torch; print('CUDA:', torch.cuda.is_available())"

# CPU kullanÄ±yorsanÄ±z, model boyutunu kÃ¼Ã§Ã¼ltÃ¼n:
# config.py'da NUM_ENCODER_LAYERS = 4 yapÄ±n
```

---

## ğŸ¯ SONRAKI ADIMLAR

Pipeline baÅŸarÄ±yla tamamlandÄ±ktan sonra:

1. **Results Ä°nceleme:**
   - `results/evaluation_report.json` â†’ Overall performance
   - `results/confusion_matrix_normalized.png` â†’ Hangi sÄ±nÄ±flar karÄ±ÅŸtÄ±rÄ±lÄ±yor?
   - `results/per_class_metrics.png` â†’ Hangi sÄ±nÄ±f daha zor?
   - `results/attention/` â†’ Model neye bakÄ±yor?

2. **Ä°yileÅŸtirmeler:**
   - Hiperparametre tuning (learning rate, batch size)
   - Daha fazla kelime ekleme (config.py â†’ TARGET_CLASS_IDS)
   - Data augmentation (config.py â†’ USE_AUGMENTATION = True)
   - Model bÃ¼yÃ¼tme/kÃ¼Ã§Ã¼ltme

3. **Deneyler:**
   - FarklÄ± pooling stratejileri (GAP vs CLS vs Last)
   - FarklÄ± model boyutlarÄ± (tiny vs small vs base vs large)
   - FarklÄ± optimizer'lar (Adam vs AdamW)

---

## ğŸ“ YARDIM

- **README.md:** Teknik detaylar ve architecture aÃ§Ä±klamasÄ±
- **RUN_PIPELINE.md:** DetaylÄ± troubleshooting
- **ilerleme.md:** Her adÄ±mÄ±n ne yaptÄ±ÄŸÄ±nÄ±n notlarÄ±

---

**ğŸ‰ HAZIRSÄ±NÄ±Z! Pipeline'Ä± Ã§alÄ±ÅŸtÄ±rmaya baÅŸlayabilirsiniz!**

