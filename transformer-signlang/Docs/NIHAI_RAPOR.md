# ğŸ¯ TRANSFORMER SIGN LANGUAGE RECOGNITION - NÄ°HAÄ° RAPOR

**Proje:** AUTSL TÃ¼rk Ä°ÅŸaret Dili TanÄ±ma Sistemi
**Model:** Transformer-based Video Classifier
**Tarih:** 9 Ekim 2025
**Durum:** TamamlandÄ± - Ä°yileÅŸtirme AÅŸamasÄ±nda

---

## ğŸ“‹ EXECUTIVE SUMMARY

### Projenin AmacÄ±

226 farklÄ± TÃ¼rk Ä°ÅŸaret Dili kelimesini video kayÄ±tlarÄ±ndan tanÄ±yabilen bir Transformer tabanlÄ± derin Ã¶ÄŸrenme modeli geliÅŸtirmek.

### Ana Bulgular

| Metrik                              | DeÄŸer   | Durum                              |
| ----------------------------------- | -------- | ---------------------------------- |
| **Training Accuracy**         | 100.00%  | âœ… MÃ¼kemmel (ama overfitting var) |
| **Validation Accuracy**       | 87.87%   | âœ… Ä°yi                            |
| **Test Accuracy (Evaluate)**  | 76.96%   | âš ï¸ Kabul edilebilir              |
| **Test Accuracy (Inference)** | 52.97%   | âŒ DÃ¼ÅŸÃ¼k                        |
| **Training Duration**         | 2:53:58  | âœ… Makul                           |
| **Model Size**                | 19.35 MB | âœ… Deployment-friendly             |
| **Total Parameters**          | 5.07M    | âœ… Optimal                         |

### Kritik Sorunlar

1. **ğŸ”´ Ciddi Overfitting**: Train-Test gap %47.03 (inference bazlÄ±)
2. **ğŸ”´ Generalization Problemi**: Evaluate (%76.96) vs Inference (%52.97) = %23.99 fark
3. **ğŸ”´ SÄ±nÄ±f DengesizliÄŸi**: 10 sÄ±nÄ±f %0 accuracy (inference)

---

## ğŸ“Š 1. DETAYLI PERFORMANS ANALÄ°ZÄ°

### 1.1 Training PerformansÄ±

#### Training Curve Ã–zeti

```
Total Epochs: 100
Best Train Acc: 100.00% (Epoch 78)
Best Val Acc: 87.87% (Epoch 98)
Best Val F1: 87.56% (Epoch 98)

Overfitting Metrikleri:
- Train-Val Gap: 12.13%
- Val-Test Gap (Evaluate): 10.91%
- Val-Test Gap (Inference): 34.90% âš ï¸ KRÄ°TÄ°K!
```

#### Epoch Ä°lerlemesi

| Phase                 | Epoch Range | Train Acc      | Val Acc        | GÃ¶zlem            |
| --------------------- | ----------- | -------------- | -------------- | ------------------ |
| **Ã–ÄŸrenme**   | 1-30        | 41.8% â†’ 99.6% | 49.1% â†’ 87.2% | HÄ±zlÄ± Ã¶ÄŸrenme  |
| **Plato**       | 31-77       | 99.6% â†’ 100%  | 87.0% â†’ 87.9% | Val plato yapmÄ±ÅŸ |
| **Overfitting** | 78-100      | 100% (sabit)   | 87.5-87.9%     | Train ezberleme    |

**TeÅŸhis:** Model Epoch 78'de train setini tamamen ezberlemiÅŸ ve artÄ±k genelleme yapmÄ±yor.

---

### 1.2 Test PerformansÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±

#### A) Evaluate.py SonuÃ§larÄ± (Preprocessed .npy files)

```
Accuracy: 76.96%
F1 (Macro): 76.19%
F1 (Weighted): 76.44%
Precision (Macro): 82.31%
Recall (Macro): 76.84%

Test Samples: 3,742
```

**SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:**

- ğŸ† MÃ¼kemmel (100% F1): 15 sÄ±nÄ±f
- â­ Ã‡ok Ä°yi (90-99% F1): 41 sÄ±nÄ±f
- âš ï¸ Orta (50-89% F1): 158 sÄ±nÄ±f
- âŒ ZayÄ±f (<50% F1): 12 sÄ±nÄ±f

#### B) Inference SonuÃ§larÄ± (Raw video files)

```
Accuracy: 52.97% âŒ ALARM!
DoÄŸru tahmin: 1,982 / 3,742
YanlÄ±ÅŸ tahmin: 1,760 / 3,742

Confidence Ä°statistikleri:
- Ortalama: 0.4704
- DoÄŸru tahminlerde: 0.6089
- YanlÄ±ÅŸ tahminlerde: 0.3144

Confidence Range:
- Min: 0.0198
- Median: 0.4817
- Max: 0.8630
```

**ğŸš¨ KRÄ°TÄ°K FARK:** Evaluate (%76.96) vs Inference (%52.97) = **%23.99 dÃ¼ÅŸÃ¼ÅŸ!**

**Muhtemel Sebepler:**

1. **Preprocessing farklÄ±lÄ±klarÄ±**: Normalize edilmiÅŸ .npy vs raw video
2. **Keypoint extraction kalitesi**: MediaPipe real-time vs offline
3. **Data distribution mismatch**: Train/test split farklÄ±lÄ±klarÄ±
4. **Padding/normalization**: Scaler consistency

---

### 1.3 SÄ±nÄ±f BazlÄ± DetaylÄ± Analiz

#### MÃ¼kemmel Performans (100% F1) - 15 SÄ±nÄ±f

```
âœ… cuzdan, dusman, eczane, gol, hastane, hayirli_olsun, 
   kolonya, komur, kopru, oda, pencere, salca, serbest, 
   uzak, yemek_pisirmek, yorgun
```

**Ortak Ã–zellikler:**

- Belirgin el hareketleri
- YÃ¼ksek temporal consistency
- Az frame variation

---

#### Tamamen BaÅŸarÄ±sÄ±z SÄ±nÄ±flar (0% Accuracy - Inference)

**Evaluate'de 0% F1:** `nasil`, `okul`, `seker` (3 sÄ±nÄ±f)

**Inference'da 0% Accuracy (yeni):**

```
âŒ saat, soylemek, neden, seker, kalem, kim, 
   nasil, tamam, leke, okul (10 sÄ±nÄ±f)
```

**KÃ¶k Neden Analizi:**

| SÄ±nÄ±f | Train Samples | Avg Zero Frames   | Problem             |
| ------- | ------------- | ----------------- | ------------------- |
| nasil   | 126           | 31.2 / 81 (38.5%) | Video Ã§ok kÄ±sa    |
| okul    | 126           | 26.4 / 81 (32.6%) | Video Ã§ok kÄ±sa    |
| seker   | 90            | 22.2 / 81 (27.4%) | Video Ã§ok kÄ±sa    |
| saat    | ~125          | ?                 | Ä°nvestigate needed |
| kalem   | ~125          | ?                 | Investigate needed  |

**Ã–nerilen Aksiyon:**

1. Bu sÄ±nÄ±flarÄ±n orijinal videolarÄ±nÄ± manuel olarak incele
2. Keypoint extraction'Ä± yeniden yap
3. Alternatif preprocessing pipeline dene
4. Data augmentation ile Ã¶rnekleri artÄ±r

---

#### YÃ¼ksek Confidence ile YanlÄ±ÅŸ Tahminler

**Top 5 En KÃ¶tÃ¼:**

| True  | Predicted | Confidence | Analiz                               |
| ----- | --------- | ---------- | ------------------------------------ |
| keske | akilsiz   | 86.30%     | Model Ã§ok emin ama yanlÄ±ÅŸ!        |
| bekar | akilsiz   | 85.30%     | `akilsiz` over-predicted           |
| olur  | bardak    | 84.81%     | `bardak` false positive yÃ¼ksek    |
| keske | akilsiz   | 84.76%     | Tekrar `akilsiz` karÄ±ÅŸÄ±klÄ±ÄŸÄ± |
| olur  | bardak    | 84.64%     | Tekrar `bardak` karÄ±ÅŸÄ±klÄ±ÄŸÄ±  |

**Pattern:**

- `akilsiz` ve `bardak` sÄ±nÄ±flarÄ± Ã§ok sÄ±k yanlÄ±ÅŸ tahmin ediliyor
- Model overconfident (yanlÄ±ÅŸ olduÄŸunda bile %85+ emin)
- **Ã‡Ã¶zÃ¼m:** Temperature scaling, label smoothing artÄ±rma

---

### 1.4 Confidence Analizi

#### DoÄŸru Tahminler (En DÃ¼ÅŸÃ¼k Confidence)

```
senet: 0.0327          â†’ Model emin deÄŸil ama doÄŸru!
nine: 0.0344
hoscakal: 0.0421
salca: 0.0429
kacmak: 0.0442
```

**Yorum:** Model bu sÄ±nÄ±flarÄ± tanÄ±yor ama confidence dÃ¼ÅŸÃ¼k â†’ Calibration sorunu

#### Confidence Distribution

```
           DoÄŸru   YanlÄ±ÅŸ
Ortalama   60.89%  31.44%
Median     61.08%  24.53%
75%        78.13%  46.21%
```

**TeÅŸhis:**

- DoÄŸru tahminlerde bile confidence ortalama %60.89 (dÃ¼ÅŸÃ¼k!)
- YanlÄ±ÅŸ tahminlerde %31.44 (Ã§ok dÃ¼ÅŸÃ¼k olmalÄ±ydÄ±)
- Model **under-confident** (doÄŸrularda) ve **over-confident** (yanlÄ±ÅŸlarda)

---

## ğŸ”´ 2. OVERFÄ°TTÄ°NG DETAYLI ANALÄ°ZÄ°

### 2.1 Overfitting Seviyesi

| KarÅŸÄ±laÅŸtÄ±rma                 | Gap              | Seviye               |
| --------------------------------- | ---------------- | -------------------- |
| Train vs Val                      | 12.13%           | ğŸŸ¡ Orta              |
| Val vs Test (Evaluate)            | 10.91%           | ğŸŸ¡ Orta              |
| **Val vs Test (Inference)** | **34.90%** | **ğŸ”´ Kritik**  |
| **Train vs Inference**      | **47.03%** | **ğŸ”´ Felaket** |

### 2.2 Overfitting Belirtileri

âœ… **Model ezberlemiÅŸ:**

- Epoch 78'de train %100'e ulaÅŸmÄ±ÅŸ
- Sonraki 22 epoch'ta train %100 kalmÄ±ÅŸ
- Val accuracy oscillate ediyor (%87.5-87.9)

âœ… **Generalization yok:**

- Train'de mÃ¼kemmel, test'te orta
- Inference'da felaket (%52.97)

âœ… **High variance:**

- AynÄ± sÄ±nÄ±fÄ±n farklÄ± Ã¶rnekleri arasÄ±nda bÃ¼yÃ¼k performance farkÄ±

### 2.3 KÃ¶k Nedenler

#### A) Model Kapasitesi vs Data Size

```
Model Parameters: 5.07M
Train Samples: 28,142
Ratio: 180 samples per 1M params â† DÃœ ÅÃœK!

Ã–nerilen: >1000 samples per 1M params
Gereken train samples: ~5M samples
Mevcut: 28K samples

TEÅHIS: Model Ã§ok bÃ¼yÃ¼k, data Ã§ok az!
```

#### B) Regularization Yetersiz

```python
# Mevcut config
DROPOUT = 0.2              # DÃ¼ÅŸÃ¼k!
LABEL_SMOOTHING = 0.1      # DÃ¼ÅŸÃ¼k!
WEIGHT_DECAY = 1e-5        # Ã‡ok dÃ¼ÅŸÃ¼k!
DATA_AUGMENTATION = None   # YOK!
```

#### C) Data Quality Issues

**Zero Frame Analizi:**

- Normal sÄ±nÄ±flar: ~15-20 zero frames (%18-25)
- Sorunlu sÄ±nÄ±flar: ~22-31 zero frames (%27-38)
- **Problem:** Padding ratio Ã§ok yÃ¼ksek, model Ã¶ÄŸrenemiyor

---

## ğŸ’¡ 3. Ä°YÄ°LEÅTÄ°RME Ã–NERÄ°LERÄ° - KAPSAMLI PLAN

### ğŸ”´ Ã–NCELÄ°K 1: ACÄ°L DÃœZELTMELER (1-2 GÃœN)

#### 1.1 Config GÃ¼ncellemesi

```python
# config.py - YENI DEÄERLER

# Regularization (GÃ¼Ã§lendirme)
DROPOUT = 0.4              # 0.2 â†’ 0.4 (2x artÄ±r)
LABEL_SMOOTHING = 0.2      # 0.1 â†’ 0.2 (2x artÄ±r)
WEIGHT_DECAY = 1e-4        # 1e-5 â†’ 1e-4 (10x artÄ±r)
GRADIENT_CLIP = 0.5        # 1.0 â†’ 0.5 (sÄ±kÄ±laÅŸtÄ±r)

# Early Stopping (Agresif)
EARLY_STOPPING_PATIENCE = 10  # 20 â†’ 10 (2x azalt)
MIN_DELTA = 0.001         # Yeni: minimum improvement threshold

# Model Architecture (KÃ¼Ã§Ã¼lt)
D_MODEL = 192             # 256 â†’ 192 (kÃ¼Ã§Ã¼lt)
NUM_ENCODER_LAYERS = 4    # 6 â†’ 4 (azalt)
NHEAD = 6                 # 8 â†’ 6 (azalt)
DIM_FEEDFORWARD = 768     # 1024 â†’ 768 (azalt)

# BEKLENEN ETKI: ~3M params (5M â†’ 3M), %40 azalma
```

**MantÄ±k:** Model Ã§ok bÃ¼yÃ¼k â†’ kÃ¼Ã§Ã¼lt, regularization Ã§ok zayÄ±f â†’ gÃ¼Ã§lendir

---

#### 1.2 Class Weights Implementation

```python
# train.py - Ana eÄŸitim scriptine ekle

from sklearn.utils.class_weight import compute_class_weight

def setup_weighted_loss(config, y_train, device):
    """
    Class imbalance iÃ§in weighted loss function
    """
    # Class weights hesapla
    unique_classes = np.unique(y_train)
    class_weights = compute_class_weight(
        class_weight='balanced',
        classes=unique_classes,
        y=y_train
    )
  
    # Extreme weights'i clip et (stability iÃ§in)
    class_weights = np.clip(class_weights, 0.5, 5.0)
  
    # Tensor'a Ã§evir
    class_weights = torch.FloatTensor(class_weights).to(device)
  
    # Loss function oluÅŸtur
    criterion = nn.CrossEntropyLoss(
        label_smoothing=config.LABEL_SMOOTHING,
        weight=class_weights
    )
  
    return criterion, class_weights

# KullanÄ±m:
criterion, weights = setup_weighted_loss(config, y_train, device)

print(f"ğŸ“Š Class Weights:")
print(f"   Min: {weights.min():.3f}")
print(f"   Max: {weights.max():.3f}")
print(f"   Mean: {weights.mean():.3f}")
```

**Beklenen Etki:** Rare class'lar daha iyi Ã¶ÄŸrenilecek, 0% F1 sÄ±nÄ±flar azalacak

---

#### 1.3 Focal Loss Implementation

```python
# models/losses.py - YENÄ° DOSYA

import torch
import torch.nn as nn
import torch.nn.functional as F

class FocalLoss(nn.Module):
    """
    Focal Loss for addressing class imbalance
  
    FL(p_t) = -Î±(1-p_t)^Î³ log(p_t)
  
    Args:
        alpha: Weighting factor (0-1)
        gamma: Focusing parameter (0-5, typically 2)
        reduction: 'mean' | 'sum' | 'none'
    """
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
  
    def forward(self, inputs, targets):
        """
        Args:
            inputs: (batch_size, num_classes) - raw logits
            targets: (batch_size,) - class indices
        """
        # Convert to probabilities
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        p_t = torch.exp(-ce_loss)
  
        # Focal term
        focal_term = (1 - p_t) ** self.gamma
  
        # Focal loss
        focal_loss = self.alpha * focal_term * ce_loss
  
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class CombinedLoss(nn.Module):
    """
    Combine Cross-Entropy and Focal Loss
    """
    def __init__(self, alpha_ce=0.5, alpha_focal=0.5, 
                 gamma=2.0, label_smoothing=0.1, class_weights=None):
        super().__init__()
        self.alpha_ce = alpha_ce
        self.alpha_focal = alpha_focal
  
        self.ce_loss = nn.CrossEntropyLoss(
            label_smoothing=label_smoothing,
            weight=class_weights
        )
        self.focal_loss = FocalLoss(gamma=gamma)
  
    def forward(self, inputs, targets):
        ce = self.ce_loss(inputs, targets)
        focal = self.focal_loss(inputs, targets)
        return self.alpha_ce * ce + self.alpha_focal * focal


# train.py'de kullanÄ±m:
from models.losses import FocalLoss, CombinedLoss

# Sadece Focal Loss
criterion = FocalLoss(alpha=0.25, gamma=2.0)

# VEYA Combine edilmiÅŸ
criterion = CombinedLoss(
    alpha_ce=0.5, 
    alpha_focal=0.5,
    gamma=2.0,
    label_smoothing=config.LABEL_SMOOTHING,
    class_weights=class_weights
)
```

**Beklenen Etki:** Hard examples daha iyi Ã¶ÄŸrenilecek, boundary decision'lar geliÅŸecek

---

### ğŸŸ  Ã–NCELÄ°K 2: DATA AUGMENTATION (2-3 GÃœN)

#### 2.1 Temporal Augmentation

```python
# utils/data_augmentation.py - YENÄ° DOSYA

import numpy as np
import torch

class TemporalAugmentation:
    """
    Temporal augmentation for sequence data
    """
    def __init__(self, 
                 jitter_range=5,
                 time_stretch_range=(0.9, 1.1),
                 time_warp_sigma=0.2,
                 dropout_prob=0.1):
        self.jitter_range = jitter_range
        self.time_stretch_range = time_stretch_range
        self.time_warp_sigma = time_warp_sigma
        self.dropout_prob = dropout_prob
  
    def random_shift(self, x):
        """Random temporal shift"""
        shift = np.random.randint(-self.jitter_range, self.jitter_range + 1)
        return np.roll(x, shift, axis=0)
  
    def time_stretch(self, x):
        """Random time stretching"""
        factor = np.random.uniform(*self.time_stretch_range)
        indices = np.arange(len(x)) * factor
        indices = np.clip(indices, 0, len(x) - 1).astype(int)
        return x[indices]
  
    def time_warp(self, x):
        """Smooth time warping"""
        warp = np.cumsum(np.random.randn(len(x)) * self.time_warp_sigma)
        warp = warp - warp.min()
        warp = warp / warp.max() * (len(x) - 1)
        indices = np.clip(warp.astype(int), 0, len(x) - 1)
        return x[indices]
  
    def frame_dropout(self, x):
        """Random frame dropout"""
        mask = np.random.rand(len(x)) > self.dropout_prob
        if mask.sum() == 0:  # En az 1 frame kalmalÄ±
            mask[0] = True
        return x[mask]
  
    def __call__(self, x):
        """Apply random augmentation"""
        aug_type = np.random.choice(['shift', 'stretch', 'warp', 'dropout', 'none'])
  
        if aug_type == 'shift':
            return self.random_shift(x)
        elif aug_type == 'stretch':
            return self.time_stretch(x)
        elif aug_type == 'warp':
            return self.time_warp(x)
        elif aug_type == 'dropout':
            return self.frame_dropout(x)
        else:
            return x


class SpatialAugmentation:
    """
    Spatial augmentation for keypoint data
    """
    def __init__(self,
                 noise_std=0.01,
                 scale_range=(0.95, 1.05),
                 rotation_deg=5):
        self.noise_std = noise_std
        self.scale_range = scale_range
        self.rotation_deg = rotation_deg
  
    def add_noise(self, x):
        """Add Gaussian noise"""
        noise = np.random.randn(*x.shape) * self.noise_std
        return x + noise
  
    def random_scale(self, x):
        """Random scaling"""
        scale = np.random.uniform(*self.scale_range)
        return x * scale
  
    def random_rotation(self, x):
        """Random rotation (2D rotation for x,y coords)"""
        angle = np.random.uniform(-self.rotation_deg, self.rotation_deg)
        rad = np.deg2rad(angle)
  
        # Reshape to (frames, keypoints, features)
        # Assuming features = [x1, y1, z1, x2, y2, z2, ...]
        # Rotate only x, y coordinates
  
        cos_a, sin_a = np.cos(rad), np.sin(rad)
        x_aug = x.copy()
  
        # Apply rotation to x,y pairs
        for i in range(0, x.shape[-1], 3):  # Assuming [x, y, z] format
            if i + 1 < x.shape[-1]:
                x_orig = x[..., i].copy()
                y_orig = x[..., i+1].copy()
          
                x_aug[..., i] = cos_a * x_orig - sin_a * y_orig
                x_aug[..., i+1] = sin_a * x_orig + cos_a * y_orig
  
        return x_aug
  
    def __call__(self, x):
        """Apply random augmentation"""
        aug_type = np.random.choice(['noise', 'scale', 'rotation', 'none'])
  
        if aug_type == 'noise':
            return self.add_noise(x)
        elif aug_type == 'scale':
            return self.random_scale(x)
        elif aug_type == 'rotation':
            return self.random_rotation(x)
        else:
            return x


# Dataset class'Ä±nda kullanÄ±m:
class AugmentedSignLanguageDataset(torch.utils.data.Dataset):
    def __init__(self, X, y, temporal_aug=None, spatial_aug=None):
        self.X = X
        self.y = y
        self.temporal_aug = temporal_aug
        self.spatial_aug = spatial_aug
  
    def __getitem__(self, idx):
        x = self.X[idx].copy()
        y = self.y[idx]
  
        # Apply augmentations
        if self.temporal_aug is not None:
            x = self.temporal_aug(x)
  
        if self.spatial_aug is not None:
            x = self.spatial_aug(x)
  
        return torch.FloatTensor(x), torch.LongTensor([y])[0]
  
    def __len__(self):
        return len(self.X)
```

**Beklenen Etki:** Diversity artÄ±ÅŸÄ±, overfitting %5-10 azalma

---

#### 2.2 Mixup/Cutmix for Sequences

```python
# utils/mixup.py - YENÄ° DOSYA

import numpy as np
import torch

def mixup_data(x, y, alpha=0.2):
    """
    Mixup augmentation for sequences
  
    Args:
        x: (batch, seq_len, features)
        y: (batch,)
        alpha: Beta distribution parameter
  
    Returns:
        mixed_x, y_a, y_b, lam
    """
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
  
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
  
    mixed_x = lam * x + (1 - lam) * x[index]
    y_a, y_b = y, y[index]
  
    return mixed_x, y_a, y_b, lam


def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """
    Mixup loss
    """
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


# train.py'de kullanÄ±m:
for epoch in range(num_epochs):
    for X_batch, y_batch in train_loader:
        # Mixup
        X_mixed, y_a, y_b, lam = mixup_data(X_batch, y_batch, alpha=0.2)
  
        # Forward
        logits = model(X_mixed)
  
        # Loss
        loss = mixup_criterion(criterion, logits, y_a, y_b, lam)
```

**Beklenen Etki:** Regularization, smooth decision boundaries

---

### ğŸŸ¡ Ã–NCELÄ°K 3: MODEL ARCHITECTURE Ä°YÄ°LEÅTÄ°RMELERÄ° (1 HAFTA)

#### 3.1 Multi-Scale Temporal Modeling

```python
# models/multi_scale_transformer.py - YENÄ° MODEL

import torch
import torch.nn as nn

class MultiScaleTransformer(nn.Module):
    """
    Multi-scale temporal transformer
    FarklÄ± temporal resolution'larda pattern'leri yakalar
    """
    def __init__(self, input_dim, num_classes, d_model=256, 
                 nhead=8, num_layers=4, scales=[1, 2, 4]):
        super().__init__()
  
        self.scales = scales
  
        # Her scale iÃ§in ayrÄ± transformer branch
        self.transformers = nn.ModuleList([
            nn.TransformerEncoder(
                nn.TransformerEncoderLayer(
                    d_model=d_model // len(scales),
                    nhead=nhead // len(scales),
                    dim_feedforward=d_model * 2,
                    dropout=0.3,
                    batch_first=True
                ),
                num_layers=num_layers
            )
            for _ in scales
        ])
  
        # Input projections
        self.input_projections = nn.ModuleList([
            nn.Linear(input_dim, d_model // len(scales))
            for _ in scales
        ])
  
        # Output head
        self.fc = nn.Linear(d_model, num_classes)
        self.dropout = nn.Dropout(0.3)
  
    def temporal_downsample(self, x, scale):
        """Downsample temporally"""
        if scale == 1:
            return x
        return x[:, ::scale, :]
  
    def forward(self, x, mask=None):
        # Process each scale
        features = []
  
        for i, scale in enumerate(self.scales):
            # Downsample
            x_scaled = self.temporal_downsample(x, scale)
      
            # Project
            x_proj = self.input_projections[i](x_scaled)
      
            # Transform
            x_trans = self.transformers[i](x_proj)
      
            # Global average pooling
            x_pooled = x_trans.mean(dim=1)
      
            features.append(x_pooled)
  
        # Concatenate multi-scale features
        x_concat = torch.cat(features, dim=1)
  
        # Dropout
        x_drop = self.dropout(x_concat)
  
        # Classification
        logits = self.fc(x_drop)
  
        return logits
```

**Beklenen Etki:** FarklÄ± temporal scale'lerde pattern yakalama, accuracy +2-3%

---

#### 3.2 Attention Visualization ve Debugging

```python
# utils/attention_analyzer.py - YENÄ° ARAÃ‡

import torch
import numpy as np
import matplotlib.pyplot as plt

class AttentionAnalyzer:
    """
    Attention pattern'leri analiz et
    """
    def __init__(self, model):
        self.model = model
        self.attention_weights = []
  
    def register_hooks(self):
        """Register hooks to capture attention"""
        def hook_fn(module, input, output):
            # MultiheadAttention output: (attn_output, attn_weights)
            if len(output) == 2:
                self.attention_weights.append(output[1].detach().cpu())
  
        # Register to all MultiheadAttention layers
        for name, module in self.model.named_modules():
            if isinstance(module, nn.MultiheadAttention):
                module.register_forward_hook(hook_fn)
  
    def analyze_attention_pattern(self, x, y_true, class_names):
        """
        Analyze attention for a single sample
        """
        self.attention_weights = []
  
        with torch.no_grad():
            logits = self.model(x.unsqueeze(0))
            y_pred = logits.argmax(dim=1).item()
  
        # Plot attention heatmaps
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.flatten()
  
        for layer_idx, attn in enumerate(self.attention_weights[:6]):
            # Average over heads
            attn_avg = attn[0].mean(dim=0).numpy()
      
            ax = axes[layer_idx]
            im = ax.imshow(attn_avg, cmap='viridis', aspect='auto')
            ax.set_title(f'Layer {layer_idx + 1}')
            ax.set_xlabel('Key Position')
            ax.set_ylabel('Query Position')
            plt.colorbar(im, ax=ax)
  
        plt.suptitle(f'True: {class_names[y_true]} | Pred: {class_names[y_pred]}')
        plt.tight_layout()
  
        return fig
  
    def find_critical_frames(self, x, top_k=10):
        """
        Find most important frames based on attention
        """
        self.attention_weights = []
  
        with torch.no_grad():
            _ = self.model(x.unsqueeze(0))
  
        # Average attention across all layers and heads
        all_attn = torch.cat([a.mean(dim=1) for a in self.attention_weights], dim=0)
        frame_importance = all_attn.mean(dim=0).mean(dim=0).numpy()
  
        # Top-k frames
        top_indices = frame_importance.argsort()[-top_k:][::-1]
  
        return top_indices, frame_importance


# KullanÄ±m:
analyzer = AttentionAnalyzer(model)
analyzer.register_hooks()

# BaÅŸarÄ±sÄ±z bir Ã¶rnek analiz et
failed_idx = np.where(~is_correct)[0][0]
fig = analyzer.analyze_attention_pattern(
    X_test[failed_idx], 
    y_test[failed_idx],
    config.CLASS_NAMES
)
plt.savefig('failed_attention.png')

# Kritik frame'leri bul
top_frames, importance = analyzer.find_critical_frames(X_test[failed_idx])
print(f"Most important frames: {top_frames}")
```

**Beklenen Etki:** Model debugging, interpretability

---

### ğŸŸ¢ Ã–NCELÄ°K 4: ENSEMBLE METHODS (2 HAFTA)

#### 4.1 Multiple Model Training

```python
# scripts/train_ensemble.py - YENÄ° SCRIPT

import torch
from config import TransformerConfig
from models.transformer_model import TransformerSignLanguageClassifier
from train import train_model

def train_ensemble(num_models=5, seeds=[42, 123, 456, 789, 101112]):
    """
    Train multiple models with different random seeds
    """
    models = []
  
    for i, seed in enumerate(seeds[:num_models]):
        print(f"\n{'='*80}")
        print(f"ğŸ¯ Training Model {i+1}/{num_models} (seed={seed})")
        print(f"{'='*80}\n")
  
        # Set seed
        torch.manual_seed(seed)
        np.random.seed(seed)
  
        # Create model
        config = TransformerConfig()
        model = TransformerSignLanguageClassifier(
            input_dim=config.INPUT_DIM,
            num_classes=config.NUM_CLASSES,
            d_model=config.D_MODEL,
            nhead=config.NHEAD,
            num_encoder_layers=config.NUM_ENCODER_LAYERS,
            dim_feedforward=config.DIM_FEEDFORWARD,
            dropout=config.DROPOUT
        )
  
        # Train
        trained_model = train_model(model, config, seed=seed)
  
        # Save
        torch.save(
            trained_model.state_dict(),
            f'checkpoints/ensemble_model_{i+1}_seed{seed}.pth'
        )
  
        models.append(trained_model)
  
    return models


class EnsemblePredictor:
    """
    Ensemble prediction with multiple models
    """
    def __init__(self, models, method='soft_voting'):
        self.models = models
        self.method = method
  
    def predict(self, x):
        """
        Args:
            x: (batch_size, seq_len, features)
  
        Returns:
            predictions: (batch_size,)
            confidence: (batch_size, num_classes)
        """
        all_logits = []
  
        with torch.no_grad():
            for model in self.models:
                model.eval()
                logits = model(x)
                all_logits.append(logits)
  
        # Stack: (num_models, batch_size, num_classes)
        all_logits = torch.stack(all_logits)
  
        if self.method == 'soft_voting':
            # Average probabilities
            probs = torch.softmax(all_logits, dim=-1)
            avg_probs = probs.mean(dim=0)
            predictions = avg_probs.argmax(dim=-1)
            confidence = avg_probs
  
        elif self.method == 'hard_voting':
            # Majority voting
            preds = all_logits.argmax(dim=-1)  # (num_models, batch_size)
            predictions = torch.mode(preds, dim=0)[0]
            confidence = None
  
        elif self.method == 'weighted_voting':
            # Weight by model confidence
            probs = torch.softmax(all_logits, dim=-1)
            max_probs = probs.max(dim=-1)[0]  # (num_models, batch_size)
            weights = max_probs / max_probs.sum(dim=0, keepdim=True)
            weighted_probs = (probs * weights.unsqueeze(-1)).sum(dim=0)
            predictions = weighted_probs.argmax(dim=-1)
            confidence = weighted_probs
  
        return predictions, confidence


# KullanÄ±m:
models = train_ensemble(num_models=5)
ensemble = EnsemblePredictor(models, method='soft_voting')

# Test
X_test_tensor = torch.FloatTensor(X_test).to(device)
predictions, confidence = ensemble.predict(X_test_tensor)
```

**Beklenen Etki:** Variance azalma, accuracy +3-5%

---

## ğŸ“‹ 4. NÄ°HAÄ° Ä°YÄ°LEÅTÄ°RME ROADMAPÄ°

### AÅŸama 1: HÄ±zlÄ± KazanÃ§lar (1-2 GÃ¼n)

```python
# config.py gÃ¼ncellemeleri
DROPOUT = 0.4                    # +0.2
LABEL_SMOOTHING = 0.2            # +0.1
WEIGHT_DECAY = 1e-4              # 10x artÄ±ÅŸ
D_MODEL = 192                    # -64 (kÃ¼Ã§Ã¼lt)
NUM_ENCODER_LAYERS = 4           # -2 (azalt)
EARLY_STOPPING_PATIENCE = 10     # -10 (sÄ±kÄ±laÅŸtÄ±r)

# Class weights ekle
# Focal Loss ekle
```

**Beklenen Ä°yileÅŸme:**

- Test Acc: 76.96% â†’ **82-85%** (+5-8%)
- Train-Val Gap: 12.13% â†’ **<8%** (-4%)
- 0% F1 sÄ±nÄ±f: 3 â†’ **0-1** (-2 sÄ±nÄ±f)

**Tahmini SÃ¼re:** 1-2 gÃ¼n (yeni training ~3 saat)

---

### AÅŸama 2: Orta Vadeli Ä°yileÅŸtirmeler (1 Hafta)

```python
# Data augmentation
- Temporal: jitter, stretch, warp, dropout
- Spatial: noise, scale, rotation
- Mixup/Cutmix

# Advanced techniques
- Multi-scale transformer
- Attention analysis
- Temperature scaling
```

**Beklenen Ä°yileÅŸme:**

- Test Acc: 82-85% â†’ **87-90%** (+5%)
- Inference Acc: 52.97% â†’ **70-75%** (+17-22%)
- Confidence calibration: Better

**Tahmini SÃ¼re:** 1 hafta

---

### AÅŸama 3: Uzun Vadeli Optimizasyon (2-3 Hafta)

```python
# Ensemble methods
- 5 model ensemble
- Soft/hard/weighted voting

# Architecture search
- Multi-scale temporal modeling
- Hybrid CNN+Transformer
- Relative positional encoding

# Data quality
- Re-extract keypoints
- Better normalization
- Manual curation of failed classes
```

**Beklenen Ä°yileÅŸme:**

- Test Acc: 87-90% â†’ **92-95%** (+5%)
- Inference Acc: 70-75% â†’ **85-88%** (+15%)
- Production-ready model

**Tahmini SÃ¼re:** 2-3 hafta

---

## ğŸ¯ 5. BEKLENEN SONUÃ‡LAR (PROJE BÄ°TÄ°MÄ°NDE)

### Hedef Metrikler

| Metric                         | Åimdi | AÅŸama 1 | AÅŸama 2 | AÅŸama 3 (Hedef) |
| ------------------------------ | ------ | -------- | -------- | ---------------- |
| **Test Acc (Evaluate)**  | 76.96% | 82-85%   | 87-90%   | **92-95%** |
| **Test Acc (Inference)** | 52.97% | 60-65%   | 70-75%   | **85-88%** |
| **Train-Val Gap**        | 12.13% | <8%      | <5%      | **<3%**    |
| **0% F1 SÄ±nÄ±f**        | 3      | 0-1      | 0        | **0**      |
| **<50% F1 SÄ±nÄ±f**      | 12     | 5-7      | 2-3      | **0-1**    |
| **Avg Confidence**       | 47.04% | 55-60%   | 65-70%   | **75-80%** |

### KPI Hedefleri

âœ… **Production Criteria:**

- Test Accuracy > 90%
- Inference Accuracy > 85%
- Overfitting Gap < 3%
- All classes F1 > 50%
- Avg confidence > 75%
- Inference speed < 100ms per video

---

## ğŸ“Š 6. KARÅILAÅTIRMALI ANALÄ°Z - Ã–ZET TABLO

### Model PerformansÄ±

| Aspect                  | Current Status      | Target         | Gap                 |
| ----------------------- | ------------------- | -------------- | ------------------- |
| **Accuracy**      |                     |                |                     |
| Train                   | 100.00%             | 95-97%         | -3-5% (overfitting) |
| Validation              | 87.87%              | 90-92%         | +2-5%               |
| Test (Evaluate)         | 76.96%              | 92-95%         | +15-18%             |
| Test (Inference)        | 52.97%              | 85-88%         | +32-35%             |
| **Overfitting**   |                     |                |                     |
| Train-Val Gap           | 12.13%              | <3%            | -9%                 |
| Val-Test Gap (Eval)     | 10.91%              | <3%            | -8%                 |
| Val-Test Gap (Infer)    | 34.90%              | <5%            | -30%                |
| **Class Balance** |                     |                |                     |
| Perfect (100% F1)       | 15/226 (6.6%)       | 50+/226 (22%)  | +35 sÄ±nÄ±f         |
| Good (>90% F1)          | 56/226 (24.8%)      | 150+/226 (66%) | +94 sÄ±nÄ±f         |
| Failed (0% F1)          | 3-10/226 (1.3-4.4%) | 0/226 (0%)     | -3-10 sÄ±nÄ±f       |
| **Confidence**    |                     |                |                     |
| Avg (All)               | 47.04%              | 75-80%         | +28-33%             |
| Avg (Correct)           | 60.89%              | 85-90%         | +24-29%             |
| Avg (Wrong)             | 31.44%              | <20%           | -11%                |

---

## ğŸ”§ 7. IMPLEMENTATION CHECKLIST

### Acil (1-2 GÃ¼n)

- [ ] Config gÃ¼ncellemesi (dropout, label smoothing, weight decay)
- [ ] Model architecture kÃ¼Ã§Ã¼ltme (d_model, layers)
- [ ] Class weights implementation
- [ ] Focal Loss implementation
- [ ] Training restart
- [ ] Results comparison

### KÄ±sa Vade (1 Hafta)

- [ ] Temporal augmentation implementation
- [ ] Spatial augmentation implementation
- [ ] Mixup/Cutmix implementation
- [ ] Failed classes data investigation
- [ ] Keypoint re-extraction (nasil, okul, seker)
- [ ] Training with augmentation
- [ ] Confusion matrix detailed analysis

### Orta Vade (2 Hafta)

- [ ] Multi-scale transformer implementation
- [ ] Attention visualization tools
- [ ] Temperature scaling for calibration
- [ ] Ensemble training (5 models)
- [ ] Ensemble inference
- [ ] Production pipeline setup

### Uzun Vade (1 Ay)

- [ ] Hybrid CNN+Transformer architecture
- [ ] Self-supervised pretraining
- [ ] Active learning for hard examples
- [ ] Manual data curation
- [ ] Deployment optimization
- [ ] Real-time inference testing

---

## ğŸ“– 8. SONUÃ‡ VE TAVSÄ°YELER

### BaÅŸarÄ±lar âœ…

1. **Model Architecture:** Transformer video temporal modeling iÃ§in uygun
2. **Training Pipeline:** BaÅŸtan sona Ã§alÄ±ÅŸÄ±yor, reproducible
3. **BazÄ± SÄ±nÄ±flar MÃ¼kemmel:** 15 sÄ±nÄ±f %100 F1, 56 sÄ±nÄ±f >%90 F1
4. **HÄ±zlÄ± Training:** 3 saatte 100 epoch
5. **Deployment-Friendly:** Model size 19MB, 5M params

### Kritik Sorunlar âŒ

1. **Ciddi Overfitting:** Train %100, Inference %52.97 (gap %47!)
2. **Generalization ZayÄ±f:** Evaluate vs Inference %24 fark
3. **SÄ±nÄ±f DengesizliÄŸi:** 10 sÄ±nÄ±f %0 accuracy
4. **Confidence Probl
